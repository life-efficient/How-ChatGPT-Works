{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Seq2seq models face a challenge that the entire representation of the encoded sequence must be captured in a single vector. That encoding represents the concept of the source sequence as a whole. \n",
    "All of the rich information in the source sequence must be captured in this \"information bottleneck\", making it likely that some detail will be lost.\n",
    "\n",
    "For a task such as translation, which a seq2seq model could tackle, this can make things difficult. The encoding gives you an idea of what the output should represent, but there are often many ways that the source could be translated, and getting a word-to-word translation can be difficult after everything has been summarised.\n",
    "\n",
    "The typical and intuitive explanation here is that a human translator does not read the whole source sentence, memorise it, and then translate it. Instead, they read the whole thing to get an idea of what the translation needs to represent, and then they translate it part by part, looking back at the source sentence to translate a few words at a time. They are primed with the concept that the translation needs to represent, but they need to pay attention to parts of the source sequence as they perform decide the next word in the translated output.\n",
    "\n",
    "## What's the result of this?\n",
    "\n",
    "Vanilla seq2seq models tend to be able to perform well on short sequences, where the information can be \"memorised\" within just a single vector, but perform worse on longer sequences.\n",
    "\n",
    "## The Attention Mechanism\n",
    "\n",
    "We can mathematically define an \"attention mechanism\".\n",
    "\n",
    "Overall, it looks like this:\n",
    "\n",
    "![attention mechanism](../images/RNN%20Seq2seq%20Attention.gif)\n",
    "\n",
    "#### Questions\n",
    "- Why do we need the decoder RNN?\n",
    "    - Different languages so makes sense to have different parameterisations\n",
    "- Why not also attend to different parts of the decoded sequence\n",
    "    - Great idea - we will get to that in self-attention\n",
    "\n",
    "- As long as the decoder contains enough information to tell it where to look back to, then it can grab more information as and when it needs it, instead of wasting effort carrying it throughout.\n",
    "\n",
    "### The Attention Score\n",
    "\n",
    "You've only got a limited amount of attention.\n",
    "But you can pay a different percentage of our attention to each word. \n",
    "The most attention we could pay to a word is 100%, and the least is 0%. \n",
    "Or 1.0 and 0.0 as proportions.\n",
    "\n",
    "So we could give each word a number between 0 and 1 which represents the proportion of our attention we give that word.\n",
    "We call this number $\\alpha_t$.\n",
    "\n",
    "\n",
    "$\\alpha$ is a vector of the attention paid to each part of the input.\n",
    "\n",
    "# $\\alpha = \\begin{bmatrix} \\alpha_1 \\\\ \\vdots \\\\ \\alpha_t \\\\ \\vdots \\\\ \\alpha_T \\end{bmatrix}$\n",
    "\n",
    "In the case of translation, $\\alpha$ has as many elements as the source sentence has tokens.\n",
    "\n",
    "# $\\alpha_t \\in \\R^T$\n",
    "\n",
    "This is the distribution of our attention paid to each input token.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we calculate the attention score?\n",
    "\n",
    "Because the attention score is a distribution, it can be computed by applying the softmax function to a vector of logits, $e$. Those logits should have larger values where more attention should be paid.\n",
    "\n",
    "# $\\alpha_t = softmax(e)$\n",
    "\n",
    "> The logits that are softmaxed to compute the attention distribution are also known as the alignment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logits = torch.tensor([12, -1, 0.4, 5, 2])\n",
    "attention_distribution = F.softmax(logits, dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So how do we compute those attention logits (the alignment scores)?\n",
    "\n",
    "Intuitively, it would make sense that the attention that should be paid to one word is a function of what we think about the output translation so far, and what we think about that word in the context of the input.\n",
    "\n",
    "That is, the current decoder hidden state, and the encoder hidden state for the timestep you're computing the attention for.\n",
    "\n",
    "We call these two things that we want to find the alignment between, queries and values:\n",
    "- Query, $Q$: \n",
    "    - The current decoder state\n",
    "    - A vector\n",
    "    - This is a current state which we want to know \n",
    "\n",
    "- Values, $V$:\n",
    "    - The encoder hidden states\n",
    "    - A set of vectors (perhaps a matrix)\n",
    "    - This is the set of representations stored in the model's \"memory\" - you can think of it like model RAM\n",
    "\n",
    "We say that the query attends to the values.\n",
    "\n",
    "$alpha_t = a(h_{decoder}^{t'-1}, h_{encoder}^t) = a(Q, V)$\n",
    "\n",
    "_Note that the most recently computed decoder hidden state is the one from the previous timestep $h_{decoder}^{t'-1}$. We don't have the current decoder hidden state - that's what we are trying to use the attention to compute._\n",
    "\n",
    "We call the function $a$, the _alignment function_. Intuitively, it tells you which parts of the source sequence correspond to the target sequence. In traditional (non-neural) NLP systems, this was a function that told you which words, if any corresponded to others between the translation pairs.\n",
    "\n",
    "### Cosing similarity alignment\n",
    "\n",
    "The simplest ways we can use these two variables is by computing their cosine similarity.\n",
    "\n",
    "\n",
    "### Could we learn the alignment scoring function too?!\n",
    "\n",
    "Assuming that the cosine similarity is the right function to compare alignment is quite an assumption.\n",
    "\n",
    "So let's learn the function instead, like we are doing for the rest of the neural network, by setting it to a trainable neural network.\n",
    "\n",
    "Typically, we use a 1-layer neural network, passing in a stacked vector of the two input hidden states.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the attention distribution\n",
    "\n",
    "The point of computing the attention distribution, which tells us which input tokens to pay attention to, was to use it to make a prediction for the next decoder hidden state.\n",
    "\n",
    "We can use it to create a sum of the encoder hidden state representations, weighted by the attention paid to each of them. \n",
    "\n",
    "This is known as the _context_ vector as it gives a representation of the hidden states in context of what should be paid attention.\n",
    "\n",
    "## $context, c = \\sum_t \\alpha_t h_{encoder}^t$\n",
    "\n",
    "# TODO diagram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(attention_distribution.shape)\n",
    "encoder_hidden_states = \n",
    "\n",
    "context = torch.dot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context is combined with the most recent decoder hidden state to compute the next decoder hidden state.\n",
    "\n",
    "## $next \\ decoder \\ hidden \\ state \\ input = \\begin{bmatrix}c \\\\ h_{decoder}^{t'}\\end{bmatrix}$\n",
    "\n",
    "## TODO diagram\n",
    "\n",
    "This is then processed as any decoder input would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-attention is the type of attention we have seen here where the values come from a different source (the encoder) than the queries (which come from the decoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention variants\n",
    "\n",
    "There are many forms of attention, but they always include:\n",
    "1. Computing the attention scores  $e \\in \\R^N$\n",
    "1. Turning this into an attention distribution, $\\alpha \\in R^N$\n",
    "1. Using the attention distribution to combine the values.\n",
    "\n",
    "Noticeable variants include\n",
    "- dot product attention\n",
    "- Multiplicative attention\n",
    "- Reduced rank multiplicative attention\n",
    "- Additive attention. Using a single-layer neural network as the alignment function instead of assuming that the cosine similarity is the right function to align values and queries with\n",
    "#### $e_t = W_{alignment} \\cdot tanh(\\begin{bmatrix}W_{encoder} \\cdot h_{decoder}^{t'-1} \\\\ \\\\ W_{decoder} \\cdot  h_{encoder}^t \\end{bmatrix})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(torch.nn.Module):\n",
    "    def __init__(self, input_dim=128):\n",
    "        attention_hidden_dim = 128\n",
    "        self.layers = torch.nn.Sequential( # TODO\n",
    "            torch.nn.Linear(2*input_dim, attention_hidden_dim), # TODO\n",
    "            torch.nn.Tanh(), # TODO\n",
    "            torch.nn.Linear(attention_hidden_dim, 2*input_dim), # TODO\n",
    "            torch.nn.Softmax() # TODO\n",
    "        ) # TODO\n",
    "\n",
    "    def forward(self, query, values):\n",
    "        alignments = self.layers(\n",
    "            torch.concat(query, values)\n",
    "        )\n",
    "        for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqWithAttention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention is a general technique that can be applied to many tasks (not just translation) and in many architectures (not just seq2seq)\n",
    "\n",
    "Attention is a way to combine any arbitrary set of representations (the values), into a fixed size representation dependent on some other representation (the query).\n",
    "\n",
    "The query is used to select the importance of the keys in the resulting summary.\n",
    "\n",
    "# TODO diagram directed graph of indexed nodes, where each node is pointed to by only itself and those before it\n",
    "\n",
    "Note that there is no notion of time (or generally position), which is why we need to encode it.\n",
    "\n",
    "This makes it different to convolution, which have an explicitly defined use of position."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention really works. It smashed benchmarks when it was discovered shortly after seq2seq.\n",
    "\n",
    "## Why does attention help?\n",
    "\n",
    "### Attention eliminates the information bottleneck\n",
    "\n",
    "At every timestep, the decoder can see the entire sequence of encoder hidden states.\n",
    "\n",
    "# TODO diagram\n",
    "\n",
    "### Attention opens the gradient superhighway\n",
    "\n",
    "Becuase of the fact that at every timestep, the entire sequence of encoder hidden states is fed directly to the decoder, the gradient does not have to flow through many sequential layers of the models to influence the weights that affected far away calculations, such as the first encoder hidden state.\n",
    "\n",
    "# TODO diagram\n",
    "\n",
    "### Attention makes the model somewhat interpretable\n",
    "\n",
    "You can tell what is being considered by looking at the attention weights\n",
    "\n",
    "# TODO diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (default, May 18 2021, 12:31:01) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffef1fb3247e42ae9cf3614f3519d4998b3b95643236a5d32641564963f5a3b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
