{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "Should all features of an example input to a neural network be treated equally? Or should we pay attention to some features more than others?\n",
    "\n",
    "Examples:\n",
    "- In language translation: we have a it feels natural to pay attention to different parts of the input as we translate\n",
    "- If you're looking to answer a question about a product from its images, it makes sense to pay attention to those that relate to the question\n",
    "\n",
    "In all of the above examples, we have:\n",
    "- A query: a representation of what we want to look for in the input values\n",
    "- A set of values: a representation of each part of the input\n",
    "- How much attention we should pay to each value\n",
    "\n",
    "## TODO generic attention mechanism\n",
    "\n",
    "Typically, the queriy and values are vector representations.\n",
    "\n",
    "E.g:\n",
    "- Seq2seq translation:\n",
    "    - Values: Encoder hidden states\n",
    "    - Query: Current decoder hidden state\n",
    "- Product image question answering:\n",
    "    - Values: Embeddings of each image\n",
    "    - Query: Question being asked\n",
    "\n",
    "## TODO generic attention examples\n",
    "\n",
    "> Attention is a way to combine an arbitrary set of representations (the values) into a single representation, in a way that pays more attention to some of them than others, based on some other representation (the query)\n",
    "\n",
    "You've only got a limited amount of attention.\n",
    "But you can pay a different percentage of our attention to each part of the input (each word, each pixel, each image). \n",
    "The most attention we could pay to a word is 100%, and the least is 0%. \n",
    "Or 1.0 and 0.0 as proportions.\n",
    "\n",
    "So we could give each word a number between 0 and 1 which represents the proportion of our attention we give that word.\n",
    "\n",
    "The query is used to select the importance of the values in the resulting summary.\n",
    "\n",
    "> Attention is another neural network building block (like all Pytorch modules)\n",
    "\n",
    "## Variations of attention\n",
    "\n",
    "Before we get into the specific details, you should understand that \n",
    "there are many forms of attention, but they always include:\n",
    "1. Align: Compute the attention scores (alignment) between the queries, $Q$ and values, $V$ $e \\in \\R^N$\n",
    "1. Weight: Turn the scores into an attention distribution, $\\alpha \\in R^N$\n",
    "1. Combine: Using the attention distribution to combine the values.\n",
    "\n",
    "Changing what happens at each of these steps change the type of attention you're implementing\n",
    "\n",
    "> We call the combined values _the context_\n",
    "\n",
    "## So how does this work mathematically?\n",
    "\n",
    "Attention assumes that you've already got the following inputs:\n",
    "- The query, which represent what you're looking for\n",
    "- The values, which represent what information you have available\n",
    "\n",
    "And expects the following output:\n",
    "- A single representation of the queried values\n",
    "\n",
    "The simplest form of attention is called dot-product attention\n",
    "\n",
    "### Dot product attention\n",
    "\n",
    "1. Align: Compute the dot product between the query and each value to compute an alignment score for each query-value pair\n",
    "1. Weight: Take the softmax of the alignment scores to compute the attention weights\n",
    "1. Combine: Add up the values, weighted by their attention weights\n",
    "\n",
    "> Notice that, in its most basic form (dot product), an attention block has no learnable parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logits = torch.tensor([12, -1, 0.4, 5, 2])\n",
    "attention_distribution = F.softmax(logits, dim=0)\n",
    "\n",
    "print(attention_distribution.shape)\n",
    "encoder_hidden_states = \n",
    "\n",
    "context = torch.dot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive Attention - Learning the alignment function\n",
    "\n",
    "Assuming that the cosine similarity is the right function to compare alignment is quite an assumption.\n",
    "\n",
    "So let's learn the function instead, like we are doing for the rest of the neural network, by setting it to a trainable neural network.\n",
    "\n",
    "Typically, we use a 1-layer neural network, passing in a stacked vector of the two input hidden states.\n",
    "\n",
    "> Using a single layer neural network as the alignment function is known as _additive attention_\n",
    "\n",
    "#### $e_t = W_{alignment} \\cdot tanh(\\begin{bmatrix}W_{encoder} \\cdot h_{decoder}^{t'-1} \\\\ \\\\ W_{decoder} \\cdot  h_{encoder}^t \\end{bmatrix})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(torch.nn.Module):\n",
    "    def __init__(self, input_dim=128):\n",
    "        attention_hidden_dim = 128\n",
    "        self.layers = torch.nn.Sequential( # TODO\n",
    "            torch.nn.Linear(2*input_dim, attention_hidden_dim), # TODO\n",
    "            torch.nn.Tanh(), # TODO\n",
    "            torch.nn.Linear(attention_hidden_dim, 2*input_dim), # TODO\n",
    "            torch.nn.Softmax() # TODO\n",
    "        ) # TODO\n",
    "\n",
    "    def forward(self, query, values):\n",
    "        alignments = self.layers(\n",
    "            torch.concat(query, values)\n",
    "        )\n",
    "        for "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other forms of attention, but those shown above are the central ones to understand right now.\n",
    "\n",
    "## Every input is connected to every output by a weight... How is this different to a linear layer in a NN, that takes a weighted input?\n",
    "\n",
    "There are similarities to a linear layer, but there are key differences:\n",
    "- The weighting of each input feature changes based on the alignment of the query and the values\n",
    "- Attention takes a set of vectors, whereas a linear layer takes in a single vector\n",
    "- In attention, the input vectors and output vector are usually the same size\n",
    "\n",
    "## Other notes about attention\n",
    "\n",
    "> Like RNNs, attention blocks can process inputs of an varying lengths\n",
    "\n",
    "> The values are the set of representations stored in the model's \"memory\" - you can think of it like model RAM\n",
    "\n",
    "> We say that the query attends to the values.\n",
    "\n",
    "We call the function $a$, the _alignment function_. Intuitively, it tells you which parts of the source sequence correspond to the target sequence. In traditional (non-neural) NLP systems, this was a function that told you which words, if any corresponded to others between the translation pairs.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Let's look at attention applied to language translation\n",
    "\n",
    "> Attention was first applied to translation problems, and was able to smash benchmarks.\n",
    "\n",
    "### Why does attention help for translation?\n",
    "\n",
    "Seq2seq models face a challenge that the entire representation of the encoded sequence must be captured in a single vector. That encoding represents the concept of the source sequence as a whole. \n",
    "All of the rich information in the source sequence must be captured in this \"information bottleneck\", making it likely that some detail will be lost.\n",
    "\n",
    "For a task such as translation, which a seq2seq model could tackle, this can make things difficult. The encoding gives you an idea of what the output should represent, but there are often many ways that the source could be translated, and getting a word-to-word translation can be difficult after everything has been summarised.\n",
    "\n",
    "The typical and intuitive explanation here is that a human translator does not read the whole source sentence, memorise it, and then translate it. Instead, they read the whole thing to get an idea of what the translation needs to represent, and then they translate it part by part, looking back at the source sentence to translate a few words at a time. They are primed with the concept that the translation needs to represent, but they need to pay attention to parts of the source sequence as they perform decide the next word in the translated output.\n",
    "\n",
    "Vanilla seq2seq models tend to be able to perform well on short sequences, where the information can be \"memorised\" within just a single vector, but perform worse on longer sequences.\n",
    "\n",
    "## The Attention Mechanism in seq2seq models\n",
    "\n",
    "So how do we compute those attention logits (the alignment scores) in a seq2seq model?\n",
    "\n",
    "Intuitively, it would make sense that the attention that should be paid to one word is a function of what we think about the output translation so far (our current decoder hidden state), and what we think about that word in the context of the input (the encoder hidden states). These will be our queries and values:\n",
    "- Query, $Q$: \n",
    "    - The current decoder state\n",
    "- Values, $V$:\n",
    "    - The encoder hidden states\n",
    "\n",
    "Overall, it looks like this:\n",
    "\n",
    "![attention mechanism](../images/RNN%20Seq2seq%20Attention.gif)\n",
    "# TODO add Q,V labels\n",
    "\n",
    "> In the case of translation, the attention distribution has as many elements as the source sentence has tokens.\n",
    "\n",
    "As long as the decoder contains enough information to tell it where to look back to, then it can grab more information as and when it needs it, instead of wasting effort carrying it throughout and making sure it is available in the final encoding.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-attention is the type of attention we have seen here where the values come from a different source (the encoder) than the queries (which come from the decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqWithAttention(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is important to understand that attention is a general technique that can be applied to many tasks (not just translation) and in many architectures (not just seq2seq)\n",
    "\n",
    "Note that there is no notion of time (or generally position), which is why we need to encode it.\n",
    "When using an RNN encoder, the values naturally contain a positional encoding, because the RNNs incorporate information from preceding timesteps.\n",
    "\n",
    "This makes it different to convolution, which have an explicitly defined use of position."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention really works. It smashed benchmarks when it was discovered shortly after seq2seq.\n",
    "\n",
    "## Why does attention help in seq2seq models?\n",
    "\n",
    "### Attention eliminates the information bottleneck\n",
    "\n",
    "At every timestep, the decoder can see the entire sequence of encoder hidden states.\n",
    "\n",
    "# TODO diagram\n",
    "\n",
    "### Attention opens the gradient superhighway\n",
    "\n",
    "Becuase of the fact that at every timestep, the entire sequence of encoder hidden states is fed directly to the decoder, the gradient does not have to flow through many sequential layers of the models to influence the weights that affected far away calculations, such as the first encoder hidden state.\n",
    "\n",
    "# TODO diagram\n",
    "\n",
    "### Attention makes the model somewhat interpretable\n",
    "\n",
    "You can tell what is being considered by looking at the attention weights\n",
    "\n",
    "# TODO diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (default, May 18 2021, 12:31:01) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffef1fb3247e42ae9cf3614f3519d4998b3b95643236a5d32641564963f5a3b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
