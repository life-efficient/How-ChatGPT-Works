{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Seq2seq models face a challenge that the entire representation of the encoded sequence must be captured in a single vector. That encoding represents the concept of the source sequence as a whole. \n",
    "\n",
    "For a task such as translation, which a seq2seq model could tackle, this can make things difficult. The encoding gives you an idea of what the output should represent, but there are often many ways that the source could be translated, and getting a word-to-word translation can be difficult after everything has been summarised.\n",
    "\n",
    "The typical and intuitive explanation here is that a human translator does not read the whole source sentence, memorise it, and then translate it. Instead, they read the whole thing to get an idea of what the translation needs to represent, and then they translate it part by part, looking back at the source sentence to translate a few words at a time. They are primed with the concept that the translation needs to represent, but they need to pay attention to parts of the source sequence as they perform decide the next word in the translated output.\n",
    "\n",
    "## What's the result of this?\n",
    "\n",
    "Vanilla seq2seq models tend to be able to perform well on short sequences, where the information can be \"memorised\" within just a single vector, but perform worse on longer sequences.\n",
    "\n",
    "## The Attention Mechanism\n",
    "\n",
    "### The Attention Score\n",
    "\n",
    "You've only got a limited amount of attention.\n",
    "But you can pay a different percentage of our attention to each word. \n",
    "The most attention we could pay to a word is 100%, and the least is 0%. \n",
    "Or 1.0 and 0.0 as proportions.\n",
    "\n",
    "So we could give each word a number between 0 and 1 which represents the proportion of our attention we give that word.\n",
    "We call this number $\\alpha_t$.\n",
    "\n",
    "\n",
    "$\\alpha$ is a vector of the attention paid to each part of the input.\n",
    "\n",
    "# $\\alpha = \\begin{bmatrix} \\alpha_1 \\\\ \\vdots \\\\ \\alpha_t \\\\ \\vdots \\\\ \\alpha_T \\end{bmatrix}$\n",
    "\n",
    "In the case of translation, $\\alpha$ has as many elements as the source sentence has tokens.\n",
    "\n",
    "# $\\alpha_t \\in \\R^T$\n",
    "\n",
    "This is the distribution of our attention paid to each input token.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we calculate the attention score?\n",
    "\n",
    "Because the attention score is a distribution, it can be computed by applying the softmax function to a vector of logits, $e$. Those logits should have larger values where more attention should be paid.\n",
    "\n",
    "# $\\alpha_t = softmax(e)$\n",
    "\n",
    "> The logits that are softmaxed to compute the attention distribution are also known as the alignment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logits = torch.tensor([12, -1, 0.4, 5, 2])\n",
    "attention_distribution = F.softmax(logits, dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So how do we compute those attention logits - the alignment scores?\n",
    "\n",
    "Intuitively, it would make sense that the attention that should be paid to one word is a function of what we think about the output translation so far, and what we think about that word in the context of the input.\n",
    "\n",
    "That is, the current decoder hidden state, and the encoder hidden state for the timestep you're computing the attention for.\n",
    "\n",
    "Note that the most recently computed decoder hidden state is the one from the previous timestep $h_{decoder}^{t'-1}$.\n",
    "We don't have the current decoder hidden state - that's what we are trying to use the attention to compute.\n",
    "\n",
    "$alpha_t = f(h_{decoder}^{t'-1}, h_{encoder}^t)$\n",
    "\n",
    "So the question is, what function is $f$? What function can combine those two vectors to give this attention logit?\n",
    "\n",
    "We could pick a function, like a multiplication or something. \n",
    "But instead, why don't we have the model figure it out for us? \n",
    "This function can be learnt, like the rest of the neural network can be learnt. \n",
    "In fact, the function itself can be a neural network.\n",
    "Typically, we use a 1-layer neural network, passing in a stacked vector of the two input hidden states.\n",
    "\n",
    "# $e_t = W_{alignment} \\cdot tanh(\\begin{bmatrix}W_{encoder} \\cdot h_{decoder}^{t'-1} \\\\ \\\\ W_{decoder} \\cdot  h_{encoder}^t \\end{bmatrix})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the attention distribution\n",
    "\n",
    "The point of computing the attention distribution, which tells us which input tokens to pay attention to, was to use it to make a prediction for the next decoder hidden state.\n",
    "\n",
    "We can use it to create a sum of the encoder hidden state representations, weighted by the attention paid to each of them. \n",
    "\n",
    "This is known as the _context_ vector as it gives a representation of the hidden states in context of what should be paid attention.\n",
    "\n",
    "## $context, c = \\sum_t \\alpha_t h_{encoder}^t$\n",
    "\n",
    "# TODO diagram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(attention_distribution.shape)\n",
    "encoder_hidden_states = \n",
    "\n",
    "context = torch.dot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context is combined with the most recent decoder hidden state to compute the next decoder hidden state.\n",
    "\n",
    "## $next \\ decoder \\ hidden \\ state \\ input = \\begin{bmatrix}c \\\\ h_{decoder}^{t'}\\end{bmatrix}$\n",
    "\n",
    "## TODO diagram\n",
    "\n",
    "This is then processed as any decoder input would be."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does attention help?\n",
    "\n",
    "### Attention eliminates the information bottleneck\n",
    "\n",
    "At every timestep, the decoder can see the entire sequence of encoder hidden states.\n",
    "\n",
    "# TODO diagram\n",
    "\n",
    "### Attention opens the gradient superhighway\n",
    "\n",
    "Becuase of the fact that at every timestep, the entire sequence of encoder hidden states is fed directly to the decoder, the gradient does not have to flow through many sequential layers of the models to influence the weights that affected far away calculations, such as the first encoder hidden state.\n",
    "\n",
    "# TODO diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffef1fb3247e42ae9cf3614f3519d4998b3b95643236a5d32641564963f5a3b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
