{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Pre-trained Machine Learning Models\n",
    "\n",
    "There are many cases where taking a pre-trained model off the shelf can be a great way to tackle a problem. \n",
    "You've seen that there are tons of models available online for tackling all kinds of problems.\n",
    "\n",
    "However, most models available online are dataset-specific or problem-specific.\n",
    "\n",
    "Generic examples:\n",
    "- Most text models are trained on a generic text dataset, but are not specialised to any particular domain\n",
    "- Many of the top image classification models are trained on [ImageNet](https://www.image-net.org/) -- which has 1000 possible classification classes -- but your problem might only have 3 different classes that you want to distinguish between. That means that the off-the-shelf model is going to have the wrong output shape.\n",
    "\n",
    "Specific examples:\n",
    "- There may be a generic image classifier online, but it's unlikely there is a model for classifying different types of bicycles in New York City\n",
    "- There may be a generic sentiment classifier online, but it's unlikely that there is one for detecting specific variations of positive sentiments in recently evolved colloquial language (slang, abbreviations, acronyms) used by a particular segment of TikTok users in comments.\n",
    "- There may be a generic text summarisation model online, but it's unlikely that there is one for summarising a specific type of medical literature on the conditions relating to human memory\n",
    "\n",
    "The more obscure or specific your problem, the less likely there's a model trained for the exact task available off the shelf.\n",
    "\n",
    "It sounds like we might have to start from scratch. But is there a middle ground?\n",
    "\n",
    "# TODO diagram of spectrum betwwen \"train from scratch\" and \"use model off the shelf\"\n",
    "\n",
    "Lots of the things that these off-the-shelf models must have learnt, are still relevant to the models that we want to produce.\n",
    "\n",
    "For example:\n",
    "- A generic image classification model trained on imagenet will have had to learnt how to recognise different types of shapes and textures. \n",
    "- A generic language model will have learnt what the general structure of sentences looks like\n",
    "\n",
    "This knowledge, stored as the parameters of the models, can be useful for our specific task, and should be transferred to our new model.\n",
    "\n",
    "This is where _model fine-tuning (or transfer learning)_ comes in.\n",
    "\n",
    "> Model fine-tuning is the process of taking a model pre-trained on a related problem as a starting point, and training it further to work for your specific case.\n",
    "\n",
    "# TODO diagram of spectrum\n",
    "\n",
    "> The starting model, which will be fine tuned, is commonly called _the backbone_\n",
    "\n",
    "E.g: \"I trained an image classifier with a ResNet-50 backbone\"\n",
    "\n",
    "Typically, the backbone is a large model that a big company spent a lot of time and compute training (check out some of Microsoft's models [here](https://huggingface.co/microsoft)).\n",
    "It may not be possible for you to train that model from scratch in your lifetime (e.g GPT-2 (yes, 2, not 3) would have taken around 13 years to train on a single GPU), but it is incredible that you can take it [straight off the shelf](https://huggingface.co/gpt2).\n",
    "\n",
    "Model fine-tuning usually consists of two steps:\n",
    "1. Replacing the model head\n",
    "1. Training the parameters of the head (and perhaps the rest of the model too)\n",
    "\n",
    "## Fine-tuning step 1: Replacing the model head\n",
    "\n",
    "> We call the part of the model that makes the final predictions from the outputs of the last hidden layer the _model head_.\n",
    "\n",
    "## TODO diagram highlighting model head\n",
    "\n",
    "> The first step in transfer learning is to replace the head of the starting model.\n",
    "\n",
    "Many people make the mistake of keeping the head, and building another layer on top of it, instead of throwing it away and replacing it.\n",
    "\n",
    "Picture this scenario:\n",
    "- You want to train a classifier to classify 3 different types of castles, starting with a classifier trained on ImageNet.\n",
    "- If you do not throw away the head, then the head will produce the same output corresponding to the class \"castle\" (a class which appears in the imagenet dataset) for every image in your specific castle dataset\n",
    "- This means that the input to your new additional layer will be the same for every example - a vector of low values everywhere except for the node corresponding to the \"castle\" logit\n",
    "- All of the information about the high level features will be compressed through this bottleneck, and the features that will help you to distinguish between different planes will be lost\n",
    "\n",
    "# TODO diagram of mistake of not replacing the model head\n",
    "\n",
    "Because of this, the original model head should be thrown away, and replaced with a new head which we will train in the next step.\n",
    "\n",
    "> Adding a new layer **on top** of the existing model head, **instead of replacing it**, is a very common mistake.\n",
    "\n",
    "# TODO diagram of replacing model head\n",
    "\n",
    "\n",
    "The only case where you do not need to replace the model head is if you are training the model on the same problem **and the same targets as the model was trained**. \n",
    "For example, if you are training an image classification model, there may be new styles of vehicles and clothing in a more recent set of images. \n",
    "This \"further training\" is typically useful because as trends change, the distribution of types of data that appears in your dataset may change. \n",
    "\n",
    "## Fine-tuning step 2: Final training\n",
    "\n",
    "Because the new model head is initialised with random parameters, you need to train them.\n",
    "\n",
    "There are two fundamental approaches to the final training phase of fine-tuning:\n",
    "1. _Weight freezing_: Keep the parameters of the original model fixed\n",
    "1. _Discriminated learning rates_: Update the parameters of both the new head and the original model, but use a lower learning rate on the original model so that the original knowledge is not lost\n",
    "\n",
    "### Fine-tuning by weight freezing\n",
    "\n",
    "Weight freezing is where you totally prevent the parameters of the original model from updating.\n",
    "\n",
    "Advantages of weight freezing:\n",
    "- The backward pass and parameter updates during training is a lot faster, because only the parameters of the head have to be updated\n",
    "    - The original model can contain +99% of the parameters\n",
    "- All of the original knowledge is retained\n",
    "\n",
    "Disadvantages of weight freezing:\n",
    "- No opportunity for most of the parameters to specialise for the new task\n",
    "- A single layer head may not have the capacity to learn the function to usefully combine the input activations into the desired output\n",
    "\n",
    "### Fine-tuning by using discriminated learning rates\n",
    "\n",
    "Using discriminated learning rates is the approach to fine tuning where you update the parameters of both the original model and the new head, but use a lower learning rate to update the original model's parameters, such that they don't change too drastically, and as such most of the knowledge is retained.\n",
    "\n",
    "Advantages of discriminated learning rates:\n",
    "- All parameters have the opportunity to specialise for the new task\n",
    "    - Earlier layers can adapt to learn representations that the new head is able to make accurate predictions from with just a single layer\n",
    "\n",
    "Disadvantages of discriminated learning rates:\n",
    "- The backward pass and parameter updates during training is a lot slower, because every parameter requires its gradient to be computed and requires updating on every optimisation step\n",
    "    - The original model can contain +99% of the parameters\n",
    "- During training, especially early on when the head is random and the predictions are bad, the high gradients can cause the model to catastrophically forget some fundamental knowledge\n",
    "\n",
    "\n",
    "## Things to be careful with when fine-tuning\n",
    "\n",
    "- There is little point in using a starting model that was trained on a very different dataset. For example, a starting model trained on ImageNet will not be a useful starting point for a model trained to classify x-rays, because many of the representations learnt by the starting model do not apply.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mFailed to start the Kernel 'ox (Python 3.10.8)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. ENOSPC: no space left on device, open '/var/folders/3z/29w5rr9d0k3_p863hm40sdnc0000gn/T/kernel-v2-61936G36SojmpgY6a.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5 (default, May 18 2021, 12:31:01) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffef1fb3247e42ae9cf3614f3519d4998b3b95643236a5d32641564963f5a3b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
