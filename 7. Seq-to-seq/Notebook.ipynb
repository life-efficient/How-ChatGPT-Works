{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is seq-to-seq modelling?\n",
    "\n",
    "Seq-to-seq (sequence-to-sequence) models are a type of neural network that can be used to map an input sequence to an output sequence. They are commonly used in natural language processing (NLP) tasks such as machine translation, language summarization, and question answering.\n",
    "\n",
    "![seq2seq intro](../images/Seq2seq%20Intro.png)\n",
    "\n",
    "Seq-to-seq models consist of two main components: an encoder and a decoder. The encoder processes the input sequence and produces a fixed-length context vector that captures the relevant information from the input. The decoder then uses the context vector to generate the output sequence.\n",
    "\n",
    "![Encoder-Decoder](../images/Encoder-Decoder.png)\n",
    "\n",
    "The encoder and decoder can be implemented using any type of neural network, such as a fully-connected network, a convolutional network, or a recurrent neural network (RNN). RNNs are particularly well-suited for seq-to-seq modelling because they can handle variable-length sequences and capture temporal dependencies.\n",
    "\n",
    "Seq-to-seq models are trained using supervised learning, where the input and output sequences are paired. During training, the model is given an input sequence and the corresponding output sequence, and it learns to predict the output sequence given the input sequence.\n",
    "\n",
    "In this lesson, we will learn how to implement a basic seq-to-seq model using PyTorch. We will start by preprocessing the data and creating a dataset object to iterate through the input and output sequences. Then, we will define the encoder and decoder models and train the seq-to-seq model using an optimizer and a loss function. Finally, we will use the trained model to generate new output sequences given input sequences.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "dataset = load_dataset(\"tatoeba\", lang1=\"en\", lang2=\"hr\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset[random.randint(0, len(dataset))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "First, let's start by preprocessing the data. We will use a dataset of English-French translations for this example. We will need to convert the data to lowercase, tokenize it, and create a vocabulary of unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "source_tokeniser = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "target_tokeniser = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\", lang=\"hr\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at how they tokenise input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = source_tokeniser.encode(\"Hello world\")\n",
    "print(encoded_example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are the tokenised sequences longer than the number of words in the sequence?\n",
    "\n",
    "Decode the sequence back into text to take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_tokeniser.decode(encoded_example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that some \"special tokens\" have been inserted around the sequence.\n",
    "\n",
    "- `[CLS]` represents the start of the sequence\n",
    "    - Its name comes from our tokeniser which is used to train BERT. During BERT's training process, the model is asked to perform some classification to better understand the text. The `[CLS]` token represents the start of a new sentence that BERT is asked to classify. It's not important to understand any more than that for now.\n",
    "    - In other tokenisers, this would be the equivalient of `[SOS]`, that more intuitively indicates the Start Of Sequence\n",
    "- `[SEP]` represents the end of the sequence\n",
    "    - Its name comes from our tokeniser which is used to train BERT. During BERT's training process, the model is asked to perform some classification to better understand the text. The `[SEP]` token represents the separation between sentences that BERT is asked to classify. It's not important to understand any more than that for now.\n",
    "    - In other tokenisers, this would be the equivalient of `[EOS]`, that more intuitively indicates the End Of Sequence\n",
    "\n",
    "Later, we will get around to generating new sequences (translations). To do that, we'll need to give the model the ids of the start of sequence token and the end of sequence token. Let's store those as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_start_of_sequence_token_id = target_tokeniser.get_vocab()[\"[CLS]\"]\n",
    "decoder_end_of_sequence_token_id = target_tokeniser.get_vocab()[\"[SEP]\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset object\n",
    "\n",
    "Now that we have preprocessed the data, let's create a dataset object to iterate through the input and output sequences. We will use a PyTorch Dataset object for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, source_lang=\"en\", target_lang=\"hr\"):\n",
    "        dataset = load_dataset(\"tatoeba\", lang1=source_lang, lang2=target_lang)\n",
    "        dataset = dataset[\"train\"]\n",
    "        self.examples = []\n",
    "        for ex in dataset:\n",
    "            ex = ex[\"translation\"]\n",
    "            \n",
    "            source_seq = ex[source_lang]\n",
    "            source_seq = source_tokeniser(source_seq)\n",
    "            source_seq = source_seq[\"input_ids\"]\n",
    "\n",
    "            target_seq = ex[target_lang]\n",
    "            target_seq = target_tokeniser(target_seq)\n",
    "            target_seq = target_seq[\"input_ids\"]\n",
    "            self.examples.append((source_seq, target_seq))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        source_seq, target_seq = example\n",
    "        return source_seq, target_seq\n",
    "\n",
    "\n",
    "def test_dataset():\n",
    "    dataset = TranslationDataset()\n",
    "    for example in dataset:\n",
    "        print(example)\n",
    "        source, target = example\n",
    "        print(source_tokeniser.decode(source))\n",
    "        print(target_tokeniser.decode(target))\n",
    "        print()\n",
    "        break\n",
    "\n",
    "\n",
    "test_dataset()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def get_dataloaders(batch_size=2):\n",
    "    dataset = TranslationDataset()\n",
    "    train_len = round(0.8*len(dataset))\n",
    "    val_len = round(0.1*len(dataset))\n",
    "    test_len = len(dataset) - val_len - train_len\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_len, val_len, test_len])\n",
    "\n",
    "    batch_size = 2\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def test_dataloaders():\n",
    "    train_loader, val_loader, test_loader = get_dataloaders()\n",
    "    for loader_name, loader in [(\"Train\", train_loader), (\"Validation\", val_loader), (\"Test\", test_loader)]:\n",
    "        for example in loader:\n",
    "            print(f\"{loader_name} loader example:\")\n",
    "            features, labels = example\n",
    "            print(\"Features\")\n",
    "            print(features)\n",
    "            print(\"Features shape:\", features.shape)\n",
    "            print(\"Labels\")\n",
    "            print(labels)\n",
    "            print(\"Label shape:\", labels.shape)\n",
    "            print()\n",
    "            break\n",
    "    \n",
    "test_dataloaders()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable length of sequences causes an error because the (source or target) sequences in a batch need to be the same length. \n",
    "That's because mathematically, tensors can't have empty values. \n",
    "We need to pad each sequence full of `[PAD]` tokens so that they have the length of the longest one.\n",
    "\n",
    "Dataloaders use their `collate_fn` to group together examples returned from your dataset. It can be set using a keyword argument upon initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_dataloaders(batch_size=2):\n",
    "    dataset = TranslationDataset()\n",
    "    train_len = round(0.8*len(dataset))\n",
    "    val_len = round(0.1*len(dataset))\n",
    "    test_len = len(dataset) - val_len - train_len\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_len, val_len, test_len])\n",
    "\n",
    "    def collate_fn(batch):    \n",
    "        source = torch.nn.utils.rnn.pad_sequence([torch.tensor(ex[0]) for ex in batch], batch_first=True)\n",
    "        target = torch.nn.utils.rnn.pad_sequence([torch.tensor(ex[1]) for ex in batch], batch_first=True)\n",
    "        return source, target\n",
    "\n",
    "    batch_size = 2\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "test_dataloaders()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder model\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, source_vocab_size, hidden_size=128, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = torch.nn.Embedding(source_vocab_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.init_hidden(len(input))\n",
    "        embedded = self.embedding(input)\n",
    "        output, self.hidden = self.gru(embedded, self.hidden)\n",
    "        return self.hidden # return the hidden state\n",
    "      \n",
    "    def init_hidden(self, batch_size):\n",
    "        self.hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "\n",
    "def test_encoder():\n",
    "    source_vocab_size = len(source_tokeniser.get_vocab())\n",
    "    target_vocab_size = len(target_tokeniser.get_vocab())\n",
    "    encoder = Encoder(source_vocab_size)\n",
    "    train_loader, _, _ = get_dataloaders()\n",
    "    for batch in train_loader:\n",
    "        inputs, _ = batch\n",
    "        encoder.init_hidden(len(inputs))\n",
    "        hidden = encoder(inputs)\n",
    "        break\n",
    "    print(\"Hidden shape:\", hidden.shape)\n",
    "\n",
    "test_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Decoding\n",
    "\n",
    "If target sequences have different lengths, then decoding them in a batch can be tricky or inefficient. It can be tricky because \n",
    "\n",
    "For simplicity, in this example, we will implement a decoder that decodes each sequence independently, instead of as part of a batch. That's going to require quite a bit more code compared to the encoder.\n",
    "\n",
    "### Decoder training vs inference\n",
    "\n",
    "When the model is being used to make predictions in the real world, the next prediction will have to continue from the previously predicted token - we have no labels in the wild!\n",
    "\n",
    "However, this can make training very difficult. That's because if a model predicts the incorrect token during decoding, and then bases the next token prediction upon that incorrect token, it's going to make it very hard to predict the correct token. As the sequence length increases, this problem gets worse. The previously incorrect tokens make it highly unlikely for the model to get anywhere close.\n",
    "\n",
    "To combat this, we can use _teacher forcing_ during training, which is where we disregard the previous predicted token, and instead pass the correct token from the labels to the model at the next timestamp.\n",
    "\n",
    "### `model.eval()` and `model.train()`\n",
    "\n",
    "These methods toggle the behaviour of child modules of a model that differ between training and evaluation.\n",
    "\n",
    "The do this by switching the `training` attribute of any `torch.nn.Module` subclass between `True` and `False`.\n",
    "\n",
    "### Where is the batch dimension needed?\n",
    "\n",
    "Recurrent PyTorch layers (`RNN`, `LSTM`, `GRU`) can process batched or unbatched examples. \n",
    "\n",
    "If the hidden state is found to be 2D ($N$ x $D$), then the model assumes that inputs are unbatched, and that they should have size $T$ x $D$.\n",
    "\n",
    "### Where is the time dimension needed?\n",
    "\n",
    "The time dimension is always expected by recurrent layers. This is true even if you're passing in individual timesteps (like we will implement below), in which case the size of that dimension should just be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, target_vocab_size, hidden_size, num_layers, start_of_sequence_token_id, end_of_sequence_token_id):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.start_of_sequence_token_id = start_of_sequence_token_id\n",
    "        self.end_of_sequence_token_id = end_of_sequence_token_id\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(target_vocab_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, num_layers, batch_first=False, bidirectional=False)\n",
    "        # TODO should this be batch first or not?\n",
    "        self.out = torch.nn.Linear(hidden_size, target_vocab_size)\n",
    "    \n",
    "    def forward(self, encodings, target_seqs=None):\n",
    "\n",
    "        if self.training:\n",
    "            assert target_seqs != None, \"The decoder requires targets in training mode to implement teacher forcing.\"\n",
    "            total_loss = 0\n",
    "        else:\n",
    "            assert target_seqs == None, \"The decoder should not receive targets in evaluation mode.\"\n",
    "\n",
    "        batch_size = encodings.shape[1]\n",
    "        decodings = []\n",
    "        for example_idx in range(batch_size):\n",
    "\n",
    "            encoding = encodings[:, example_idx, :]\n",
    "            \n",
    "            if self.training:\n",
    "                target_seq = target_seqs[example_idx]\n",
    "                decoding, loss = self.forward_single_example(encoding, target_seq)\n",
    "                total_loss += loss\n",
    "            else:\n",
    "                decoding = self.forward_single_example(encoding)\n",
    "            decodings.append(decoding)\n",
    "\n",
    "        if self.training:\n",
    "            return decodings, loss\n",
    "        else:\n",
    "            return decodings\n",
    "\n",
    "    # def eval(self):\n",
    "    #     super().eval() # do everything that the parent model would do\n",
    "    #     self.training = False\n",
    "    \n",
    "    # def train(self):\n",
    "    #     super().train()\n",
    "    #     self.training = True\n",
    "\n",
    "    def forward_single_example(self, encoding, target_seq=None):\n",
    "\n",
    "        if self.training:\n",
    "            assert target_seq != None, \"The decoder requires targets in training mode to implement teacher forcing.\"\n",
    "        else:\n",
    "            assert target_seq == None, \"The decoder should not receive targets in evaluation mode.\"\n",
    "\n",
    "        current_token_id = self.start_of_sequence_token_id\n",
    "        current_token_id = torch.tensor(current_token_id)\n",
    "        current_token_id = current_token_id.unsqueeze(0)\n",
    "        # embedding = embedding.unsqueeze(0) # add expected time dimension\n",
    "\n",
    "        # encoding = encoding.unsqueeze(1)\n",
    "        self.hidden = encoding\n",
    "\n",
    "        if self.training:\n",
    "            loss = 0\n",
    "\n",
    "        predicted_sequence = []\n",
    "        while True:\n",
    "\n",
    "            # Predict next token\n",
    "            embedding = self.embedding(current_token_id)\n",
    "            output, self.hidden = self.gru(embedding, self.hidden)\n",
    "            output = self.out(output)\n",
    "\n",
    "            # Calculate loss if in training mode\n",
    "            if self.training:\n",
    "                target_token_id = target_seq[0]\n",
    "                target_seq = target_seq[1:]\n",
    "                # print(output.shape)\n",
    "                # print(target_token_id.shape)\n",
    "                target_token_id = target_token_id.unsqueeze(0)\n",
    "                loss += F.cross_entropy(output, target_token_id)\n",
    "\n",
    "            current_token_id = torch.argmax(output, dim=1)\n",
    "            predicted_sequence.append(current_token_id.item())\n",
    "\n",
    "            # Implement teacher forcing if in training mode\n",
    "            if self.training:\n",
    "                current_token_id = target_token_id\n",
    "\n",
    "            # Stopping conditions\n",
    "            if len(predicted_sequence) > 10:\n",
    "                break\n",
    "            elif current_token_id == self.end_of_sequence_token_id:\n",
    "                break\n",
    "        if self.training:\n",
    "            # loss /= len(target_seq) # Normalise by sequence length\n",
    "            return predicted_sequence, loss\n",
    "        else:\n",
    "            return predicted_sequence\n",
    "    \n",
    "    # def init_hidden(self):\n",
    "    #     self.hidden = torch.zeros(self.num_layers, self.hidden_size) # Does not need batch dimension as it will process unbatched examples\n",
    "\n",
    "def test_decoder():\n",
    "    source_vocab_size = len(source_tokeniser.get_vocab())\n",
    "    target_vocab_size = len(target_tokeniser.get_vocab())\n",
    "    hidden_size = 128\n",
    "    num_layers = 3\n",
    "    encoder = Encoder(source_vocab_size, hidden_size, num_layers)\n",
    "    decoder = Decoder(target_vocab_size, hidden_size, num_layers, decoder_start_of_sequence_token_id, decoder_end_of_sequence_token_id)\n",
    "\n",
    "    max_target_seq_len = 20\n",
    "\n",
    "    train_loader, _, _ = get_dataloaders()\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        encoder.init_hidden(len(inputs))\n",
    "        hidden = encoder(inputs)\n",
    "\n",
    "        # Test training mode\n",
    "        decoder.train()\n",
    "        predicted_seq, loss = decoder(hidden, targets)\n",
    "        print(\"Training mode tests passed\")\n",
    "\n",
    "        # Test evaluation mode\n",
    "        print(decoder.training)\n",
    "        decoder.eval()\n",
    "        print(decoder.training)\n",
    "        predicted_seq = decoder(hidden)\n",
    "        print(\"Testing mode tests passed\")\n",
    "\n",
    "        # for example_idx in range(len(inputs)):\n",
    "        #     decoder.hidden = hidden[:, example_idx, :].unsqueeze(1)\n",
    "\n",
    "        #     current_token_id = target_tokeniser.get_vocab()[\"[CLS]\"]\n",
    "\n",
    "        #     predicted_sequence = [current_token_id]\n",
    "        #     for idx in range(max_target_seq_len):\n",
    "        #         current_token_id = torch.tensor(current_token_id).view(1, 1, 1)\n",
    "        #         prediction = decoder(current_token_id)\n",
    "        #         current_token_id = torch.argmax(prediction, dim=2)\n",
    "        #         predicted_sequence.append(current_token_id.item())\n",
    "        #         if current_token_id == target_tokeniser.get_vocab()[\"[SEP]\"]:\n",
    "        #             break\n",
    "        #     print(predicted_sequence)\n",
    "        break\n",
    "    print(\"Hidden shape:\", hidden.shape)\n",
    "\n",
    "test_decoder()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq = Encoder + Decoder\n",
    "\n",
    "Now we need to combine the encoder and decoder into a single model that encodes the source sequence, then decodes it into a target sequence.\n",
    "\n",
    "This should be pretty simple. The Seq2Seq model simply needs to:\n",
    "- Implement an initialiser\n",
    "    - Initialise an encoder\n",
    "    - Initialise a decoder\n",
    "- Implement the forward pass where:\n",
    "    - The source sequence is passed through the encoder to produce an encoding\n",
    "    - The encoding is passed through the decoder to produce a prediction of the target sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(torch.nn.Module):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size, num_layers, hidden_size, decoder_start_of_sequence_token_id, decoder_end_of_sequence_token_id):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(source_vocab_size, hidden_size, num_layers)\n",
    "        self.decoder = Decoder(target_vocab_size, hidden_size, num_layers, decoder_start_of_sequence_token_id, decoder_end_of_sequence_token_id)\n",
    "        self.eval() # Default to evaluation mode\n",
    "\n",
    "    def forward(self, source_seqs, target_seqs=None):\n",
    "        batch_size = len(source_seqs) # TODO you could eliminate this by randomly sampling from the dataset instead of indexing\n",
    "        encoding = self.encoder(source_seqs)\n",
    "        if self.training:\n",
    "            assert target_seqs != None, \"The seq2seq model requires targets in training mode to implement teacher forcing.\"\n",
    "            target_seq, loss = self.decoder(encoding, target_seqs)\n",
    "            return target_seq, loss\n",
    "        else:\n",
    "            target_seq = self.decoder(encoding)\n",
    "            return target_seq\n",
    "\n",
    "def test_seq2seq_model():\n",
    "    source_vocab_size = len(source_tokeniser.get_vocab())\n",
    "    target_vocab_size = len(target_tokeniser.get_vocab())\n",
    "    hidden_size = 128\n",
    "    num_layers = 3\n",
    "    seq2seq = Seq2Seq(source_vocab_size, target_vocab_size, num_layers, hidden_size, decoder_start_of_sequence_token_id, decoder_end_of_sequence_token_id)\n",
    "    train_loader, _, _ = get_dataloaders()\n",
    "    for batch in train_loader:\n",
    "        source_seqs, target_seqs = batch\n",
    "        predicted_target_seqs = seq2seq(source_seqs)\n",
    "        print(\"Initially random sequences generated:\")\n",
    "        for seq in predicted_target_seqs:\n",
    "            print(target_tokeniser.decode(seq))\n",
    "        break\n",
    "    print(\"Seq2Seq model tests passed\")\n",
    "\n",
    "test_seq2seq_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "source_vocab_size = len(source_tokeniser.get_vocab())\n",
    "target_vocab_size = len(target_tokeniser.get_vocab())\n",
    "\n",
    "def train(model, dataloader, hparam_dict, lr=0.01, epochs=1):\n",
    "    model.train()    \n",
    "    batch_idx = 1\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    running_avg = None\n",
    "\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            source_seqs, target_seqs = batch\n",
    "            \n",
    "            # Forward pass\n",
    "            prediction, loss = model(source_seqs, target_seqs)\n",
    "\n",
    "            # Log loss\n",
    "\n",
    "            writer.add_scalar(\"Loss/Train\", loss.item(), batch_idx)\n",
    "            running_avg = running_avg + ((loss.item() - running_avg) / batch_idx) if running_avg != None else loss.item()\n",
    "            if batch_idx % 100 == 0:\n",
    "                writer.add_hparams(hparam_dict, {f\"hparam/{batch_idx}-step loss\": running_avg})\n",
    "\n",
    "            for source_seq, prediction_seq, target_seq in zip(source_seqs, prediction, target_seqs): \n",
    "                source_seq = source_tokeniser.decode(source_seq)\n",
    "                prediction = target_tokeniser.decode(prediction_seq)\n",
    "                target_seq = target_tokeniser.decode(target_seq)\n",
    "                writer.add_text(\n",
    "                    \"Text\",\n",
    "                    f\"\"\"\n",
    "                    Source:    {source_seq}\n",
    "                    Predicted: {prediction}\n",
    "                    Label:     {target_seq}\n",
    "                    \"\"\", \n",
    "                    batch_idx\n",
    "                )\n",
    "            batch_idx += 1\n",
    "            # print(\"Loss:\", loss.item())\n",
    "\n",
    "            # Do optimisation\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "def test_train():\n",
    "    source_vocab_size = len(source_tokeniser.get_vocab())\n",
    "    target_vocab_size = len(target_tokeniser.get_vocab())\n",
    "    hidden_size = 256\n",
    "    num_layers = 3\n",
    "    batch_size = 8\n",
    "    lr = 0.01\n",
    "\n",
    "    hparam_dict = {\n",
    "        \"hidden_size\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": lr\n",
    "    }\n",
    "\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(batch_size=batch_size)\n",
    "    model = Seq2Seq(source_vocab_size, target_vocab_size, num_layers, hidden_size, decoder_start_of_sequence_token_id, decoder_end_of_sequence_token_id)\n",
    "    \n",
    "    train(model, train_loader, hparam_dict)\n",
    "\n",
    "test_train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trains, but it performs poorly. If you think about it, this is a really tough job because:\n",
    "1. The network is pretty small\n",
    "1. We have not trained it for very long\n",
    "1. The batch size is very small (for example purposes, because larger batches take longer to train on)\n",
    "1. Alongside translation, the network is simultaneously having to learn embeddings for words in both languages!\n",
    "\n",
    "Let's tackle the last issue mentioned. When we initialise our seq2seq model, the word embeddings are random and meaningless! To address this, we could use pre-trained word embeddings from an off-the shelf model, such as BERT. This will allow us to start off with useful word representations that we can either freeze, or fine-tune further during training.\n",
    "\n",
    "As you can tell from the results, translation is tough! If you think about what the model needs to have an understanding of to tackle the translation, you can break it down into parts. To do translation, the model needs to learn:\n",
    "\n",
    "1. The word representations of the source language\n",
    "1. The word representations of the target language\n",
    "1. A model of the source language\n",
    "1. A model of the target language\n",
    "1. A language agnostic encoding\n",
    "\n",
    "All of these tasks are solved by some parameterisation of a part of the model. But initially, all of our parameters are random. \n",
    "\n",
    "Is there a way we could easily get better initial parameters?\n",
    "\n",
    "> Yes\n",
    "\n",
    "Most of the tasks mentioned above can solved by independent models. Training those models will produce pre-trained parameters.\n",
    "\n",
    "> Note that for this translation example we could probably pulled a working translation model off the shelf too, rather than just the word embeddings, but 1) we wouldn't learn much by doing that and 2) in some cases the language pair model or specific dataset (e.g. translation of medical literature) may not exist.\n",
    "\n",
    "## Pre-trained Word Embeddings\n",
    "\n",
    "We can eliminate steps 1 & 2 by using pre-trained word embeddings. These pre-trained word embeddings could be generated by word2vec or BERT and taken off the shelf.\n",
    "\n",
    "Let's use the pre-trained BERT models for these languages as initial word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we do more pre-training?\n",
    "\n",
    "Our model still has to learn:\n",
    "1. A model of the source language\n",
    "1. A model of the target language\n",
    "1. A language agnostic encoding\n",
    "\n",
    "Can we eliminate any more of those tasks?\n",
    "\n",
    "## We can do more pre-training by pre-training the language models!\n",
    "\n",
    "Instead of starting the encoder and decoder off with random parameters, we can train a source and target language model to get a good starting set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 08:09:04) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3e643153f50b336c1c7a9d4d544c5113a86fd55c72312d55d3acd153a8b13ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
