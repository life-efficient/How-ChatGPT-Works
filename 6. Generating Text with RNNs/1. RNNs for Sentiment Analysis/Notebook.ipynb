{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing an RNN for Sentiment Classification\n",
    "\n",
    "> Many-to-one recurrent neural networks can be used for sentiment classification. They take in an input sequence, then output a probability distribution over possible classes.\n",
    "\n",
    "![](./images/RNN%20Text%20Classifier.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need some libraries before we start implementing this architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (2.6.1)\n",
      "Requirement already satisfied: packaging in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: pandas in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: dill<0.3.7 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: aiohttp in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: xxhash in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from datasets) (0.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from datasets) (1.21.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->datasets) (4.3.0)\n",
      "Requirement already satisfied: filelock in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.6)\n",
      "Requirement already satisfied: six in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: transformers in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (4.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: filelock in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: requests in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: torch in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (1.12.1)\n",
      "Requirement already satisfied: typing_extensions in /Users/ice/opt/miniconda3/lib/python3.9/site-packages (from torch) (4.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find a classic sentiment classification dataset [here](https://huggingface.co/datasets/carblacac/twitter-sentiment-analysis) on HuggingFace and start using it as shown in the snippet on that page, copied in below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration None\n",
      "Found cached dataset twitter-sentiment-analysis (/Users/ice/.cache/huggingface/datasets/carblacac___twitter-sentiment-analysis/None/1.0.0/cd65e23e456de6a4f7264e305380b0ffe804d6f5bfd361c0ec0f68d8d1fab95b)\n",
      "100%|██████████| 3/3 [00:00<00:00, 68.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"carblacac/twitter-sentiment-analysis\", \"None\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'feeling'],\n",
      "        num_rows: 119988\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'feeling'],\n",
      "        num_rows: 29997\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'feeling'],\n",
      "        num_rows: 61998\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a dictionary-looking object, with three keys, each mapping to the train, validation, or test split of the dataset. We'll work with the training set split until later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'feeling'],\n",
      "    num_rows: 119988\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_set = dataset[\"train\"]\n",
    "print(train_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '@fa6ami86 so happy that salman won.  btw the 14sec clip is truely a teaser', 'feeling': 0}\n"
     ]
    }
   ],
   "source": [
    "for example in train_set:\n",
    "  print(example)\n",
    "  break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that it's a dictionary with two keys `text` and `feeling`.\n",
    "\n",
    "As described in the [documentation](https://huggingface.co/datasets/carblacac/twitter-sentiment-analysis), the `feeling` is a binary value, zero or one. A zero indicates that the text of the tweet is negative. A one indicates that the text of the tweet is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " When the dataset was being created, someone manually labelled '@fa6ami86 so happy that salman won.  btw the 14sec clip is truely a teaser' as negative\n"
     ]
    }
   ],
   "source": [
    "idx_to_sentiment = {\n",
    "    0: \"negative\",\n",
    "    1: \"positive\"\n",
    "}\n",
    "\n",
    "example_tweet = example[\"text\"]\n",
    "example_sentiment = example[\"feeling\"]\n",
    "\n",
    "print(f\" When the dataset was being created, someone manually labelled '{example_tweet}' as {idx_to_sentiment[example_sentiment]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our raw text and integer sentiment labels, we need to tokenise the text. We can do this using a pre-trained tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 1.28MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2023, 2003, 2061, 10990, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "# TOKENISE\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentence = \"This is so exciting!\"\n",
    "\n",
    "print(tokenizer.encode(sentence))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@fa6ami86 so happy that salman won.  btw the 14sec clip is truely a teaser\n",
      "tensor([  101,  1030,  6904,  2575, 10631, 20842,  2061,  3407,  2008, 28542,\n",
      "         2180,  1012, 18411,  2860,  1996,  2403,  3366,  2278, 12528,  2003,\n",
      "         2995,  2135,  1037, 27071,   102])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # TODO remove twitter handles\n",
    "    text = tokenizer.encode(text) # TODO tokenise\n",
    "    text = torch.tensor(text) # TODO cast to torch tensor\n",
    "    return text\n",
    "\n",
    "\n",
    "example_text = example[\"text\"]\n",
    "print(example_text)\n",
    "processed_text = preprocess_text(example_text)\n",
    "print(processed_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is ready, let's build the RNN classification model.\n",
    "\n",
    "Before diving in, it's important to understand that PyTorch's RNN layer works a little differently to other layers:\n",
    "- It outputs two tensors in a tuple `(out, hidden)`\n",
    "    - `out` contains: \n",
    "        - the hidden state of the last RNN layer, for every timestep\n",
    "    - `hidden` contains:\n",
    "        - the hidden state of the all RNN layers, for the last timestep\n",
    "\n",
    "Why does the RNN layer have that output?\n",
    "- Depending on the problem you're tackling, you may need different things\n",
    "- Problems where RNNs output a sequence, need the hidden state for every timestep\n",
    "- In some problems, where you want to use the internal state to represent something, like an embedding of some sequence data, use the hidden states as that representation\n",
    "\n",
    "> RNNs break one of my favourite PyTorch rules: \"the first dimension is the batch dimension\". Instead, the first dimension is by default the time (sequence position) dimension\n",
    "\n",
    "![](./images/PyTorch%20RNN%20Outputs.png)\n",
    "\n",
    "Taking a look at the input and output parameter sizes for each layer in the [docs](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) will be usful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4992], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "  def __init__(self, vocab_size, hidden_size, num_layers=2):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_size : int\n",
    "      The number of different words that the RNN needs to be able to embed\n",
    "\n",
    "    hidden_size : int\n",
    "      The size of the internal vector representation in each layer of the RNN\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.embedding = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "    self.rnn = torch.nn.RNN(hidden_size, hidden_size, num_layers=num_layers, bidirectional=False)\n",
    "    self.linear = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "  def forward(self, X):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch.tensor Size (T, B, D)\n",
    "      A single example's tensor of input sequence tokens\n",
    "    \"\"\"\n",
    "    embedding = self.embedding(X) # TODO get embedding of input\n",
    "    \n",
    "    outputs, final_hidden = self.rnn(embedding)\n",
    "\n",
    "    final_output = outputs[-1]\n",
    "    logit = self.linear(final_output)\n",
    "    # print(logit)\n",
    "    probability = torch.sigmoid(logit)\n",
    "    return probability\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "\n",
    "test_example = preprocess_text(example[\"text\"])\n",
    "\n",
    "rnn = RNN(vocab_size, hidden_size=128)\n",
    "prediction = rnn(test_example)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to implement the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3z/29w5rr9d0k3_p863hm40sdnc0000gn/T/ipykernel_33697/1840360979.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/3z/29w5rr9d0k3_p863hm40sdnc0000gn/T/ipykernel_33697/1840360979.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;31m# CALCULATE GRADIENTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;31m# MOVE PARAMETERS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(model, dataset, epochs=10):\n",
    "\n",
    "  # INITIALISE TRACKING\n",
    "  writer = SummaryWriter()\n",
    "  batch_idx = 0\n",
    "\n",
    "  optimiser = torch.optim.SGD(model.parameters(), lr=0.01) # Define optimiser and set learning rate\n",
    "  \n",
    "  for epoch in range(epochs):# a number of times\n",
    "    for example in dataset: # iterate throguh the dataset\n",
    "      \n",
    "      # UNPACK EXAMPLE\n",
    "      tweet = example[\"text\"]\n",
    "      sentiment = example[\"feeling\"]\n",
    "      \n",
    "      # PREPROCESS TEXT\n",
    "      token_idxs = preprocess_text(tweet)\n",
    "      \n",
    "      # torch tensor and resize\n",
    "      sentiment = torch.tensor(sentiment)\n",
    "      \n",
    "      # MAKE PREDICTION\n",
    "      token_idxs = token_idxs.unsqueeze(1) # (T, B, D)\n",
    "      prediction = model(token_idxs) # make prediction\n",
    "      \n",
    "      # CALCULATE LOSS\n",
    "      loss = F.binary_cross_entropy(prediction.squeeze(), sentiment.float()) # calculate loss\n",
    "      \n",
    "      # CALCULATE GRADIENTS\n",
    "      loss.backward()\n",
    "      \n",
    "      # MOVE PARAMETERS\n",
    "      optimiser.step()\n",
    "\n",
    "      # ZERO GRAD\n",
    "      optimiser.zero_grad()\n",
    "\n",
    "      # TRACK PROGRESS\n",
    "      writer.add_scalar(\"Train/Loss\", loss.item(), batch_idx)\n",
    "      batch_idx += 1\n",
    "\n",
    "rnn = RNN(vocab_size, hidden_size=128, num_layers=2)\n",
    "train(rnn, train_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It runs! But the training curves don't look great. \n",
    "\n",
    "There are a few key things we can do to improve quickly:\n",
    "- Batch examples\n",
    "    - Currently each parameter update is based on what's best for just a single example, which almost certainly isn't going to be representative of the update that would do best for all of the examples in general!\n",
    "- Use pre-trained word embeddings\n",
    "- Explore other hyperparameters including:\n",
    "    - Increase the hidden size\n",
    "    - Tune the optimiser and its learning rate\n",
    "    - Tune the embedding size\n",
    "\n",
    "\n",
    "The details of these things are rather intricate, and the utility of them has been reduced by models such as the transformer, which you should check out next if you're interested in improving the performance here.\n",
    "\n",
    "Nevertheless, it's essential to understand how RNNs can be used for classification to build upon this knowledge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffef1fb3247e42ae9cf3614f3519d4998b3b95643236a5d32641564963f5a3b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
