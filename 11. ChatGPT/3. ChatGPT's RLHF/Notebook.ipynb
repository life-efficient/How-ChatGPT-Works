{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How ChatGPT Works Part 3: RLHF\n",
    "\n",
    "> Reinforcement Learning with Human Feedback, or RLHF, is a technique used to update a machine learning model based on human feedback\n",
    "\n",
    "## PPO\n",
    "\n",
    "## RL Objective Function\n",
    "\n",
    "- Averaged over a batch of different responses\n",
    "- Rewards ratio: /frac{reward with new params}{reward with old params} for the same input prompt    \n",
    "- Multiplied by the advantage function\n",
    "- Clipped to not change the policy too much - so that the new policy is in proximity of the other in terms of how much the reward will change\n",
    "- Either:\n",
    "\n",
    "\n",
    "The reward model trained earlier is used to determine the score\n",
    "\n",
    "## What if the reward model is wrong?\n",
    "\n",
    "The policy is optimised to maximise the reward model score.\n",
    "\n",
    "That means everything depends on the reward model being accurate.\n",
    "\n",
    "Assuming that the reward model is accurate, with too much fine tuning via RLHF, the policy can begin to overfit to the reward model and in fact produce responses less preferred by humans.\n",
    "\n",
    "## Further Details\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
