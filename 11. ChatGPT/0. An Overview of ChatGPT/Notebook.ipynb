{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Overview of ChatGPT\n",
    "\n",
    "## Introduction\n",
    "\n",
    "> ChatGPT is an AI system that can engage in back and forth conversational interactions in a chatbot-style interface. It is capable of writing code, correcting or adjusting its responses based on feedback from the user.\n",
    "\n",
    "ChatGPT is a complicated system that builds on top of a large language model, like GPT-3. The sum of this amounts to a breakthrough that has truly democratised AI by making it available in an intuitive interface that anyone can use.\n",
    "\n",
    "ChatGPT is certainly capable of making mistakes, including:\n",
    "- Providing infactual information\n",
    "- Producing biased responses\n",
    "- Sometimes (although more and more rarely) producing inappropriate or harmful responses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How ChatGPT Works\n",
    "\n",
    "So how does it work under the hood?\n",
    "\n",
    "ChatGPT is implemented in 3 steps, as shown below:\n",
    "\n",
    "![](./images/How%20chatGPT%20is%20trained.png)\n",
    "\n",
    "1. Supervised Fine-Tuning (SFT): Fine tune a pre-trained language model (GPT-3.5) to act like a chatbot\n",
    "2. The Reward Model (RM): Train a new _reward model_ to identify which responses generated by the chatbot are better than others\n",
    "3. Reinforcement Learning with Human Feedback (RLHF): Use the reward model to score generated responses, and update the language model to prefer responses with a higher score\n",
    "\n",
    "> The point of SFT and RLHF are to make the language model better and more aligned with human intention. The RM is a necessary component to do RLHF.\n",
    "\n",
    "## InstructGPT\n",
    "\n",
    "Before the development and release of ChatGPT, this method was used to produce a model called InstructGPT, a language model based on GPT-3 which interprets prompts as instructions rather than as some text that needs continuing on from. \n",
    "This makes the models more easy to interact with because you can just give them commands, instead of having to do prompt engineering.\n",
    "\n",
    "Nowadays, all models deployed on the OpenAI API use the InstructGPT variant.\n",
    "\n",
    "### InstructGPT Results:\n",
    "\n",
    "As reported in the [paper on InstructGPT](https://arxiv.org/pdf/2203.02155.pdf)\n",
    "- Labelers significantly prefer InstructGPT outputs over outputs from GPT-3\n",
    "- InstructGPT models show improvements in truthfulness over GPT-3\n",
    "- InstructGPT shows small improvements in toxicity over GPT-3, but not bias\n",
    "- InstructGPT still makes simple mistakes\n",
    "- And more\n",
    "\n",
    "Aside from that, it's clear how ChatGPT has become extremely useful in many use cases by following the same training approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "As described in their [paper](https://arxiv.org/pdf/2203.02155.pdf), to collect data to fine tune the very initial InstructGPT models, OpenAI had human labellers create prompts. The three requested prompt types were:\n",
    "- Plain: Ask the labelers to come up with an arbitrary task, while ensuring the\n",
    "tasks had sufficient diversity.\n",
    "- Few-shot: Ask the labelers to come up with an instruction, and multiple query/response\n",
    "pairs for that instruction\n",
    "- User-based: Ask labelers to come up with prompts corresponding to use-cases stated in waitlist applications to the OpenAI\n",
    "API.\n",
    "\n",
    "These manually created prompts led to three datasets, used for the three stages of training:\n",
    "- The supervised fine-tuning (SFT) dataset\n",
    "    - Features: Prompts\n",
    "    - Labels: Ideal responses\n",
    "- The reward model (RM) dataset\n",
    "    - Features: Prompts & responses\n",
    "    - Labels: Rankings of each response \n",
    "- The PPO dataset\n",
    "    - Features: Prompts & responses\n",
    "    - No labels\n",
    "\n",
    "To create the datasets for the original InstructGPT models, OpenAI hired a team of 40 labellers from [Upwork](https://www.upwork.com/) and used [Scale AI](https://scale.com/rlhf) to manage the datasets.\n",
    "\n",
    "## Supervised Fine-Tuning (SFT)\n",
    "\n",
    "> Like RLHF, the point of supervised fine-tuning is to make the language model better and more aligned with human intention\n",
    "\n",
    "> SFT requires a labelled dataset\n",
    "\n",
    "> Up to a point, overfitting the model to the SFT dataset can continue to increase human preference ratings.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Collect demonstration data and train a supervised policy\n",
    "\n",
    "This means that a team of humans literally write out acceptable responses to a range of prompts. These responses are saved, and make up the raw data for a dataset.\n",
    "\n",
    "The final part of step 1 is to _train a supervised policy_.\n",
    "\n",
    "> A policy is something that defines how you act in a certain situation. In the case of ChatGPT, the \"situation\" is the instruction written by the user (or the conversation so far), and the policy defines what response ChatGPT should produce.\n",
    "\n",
    "The policy is determined by the parameters of the language model. Initially, this policy is defined by the parameters of the model used as a starting point (the backbone)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning with Human Feedback\n",
    "\n",
    "### Recap: What is Reinforcement Learning?\n",
    "\n",
    "> Reinforcement learning is where \n",
    "\n",
    "### No Labels, No Problem\n",
    "\n",
    "The fine-tuned model can use the reward model (RM) to evaluate new generated text, without requiring labelled ideal responses.\n",
    "\n",
    "> RLHF does not require the prompts in its dataset to be labelled with ideal responses, but it does require the reward model (RM)\n",
    "\n",
    "### Using the reward model\n",
    "\n",
    "> The reward model can be used to produce a reward for the fine-tuned model in a reinforcement learning setup.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
