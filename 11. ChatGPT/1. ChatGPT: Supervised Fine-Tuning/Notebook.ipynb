{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How ChatGPT Works Part 1: Supervised Fine-Tuning\n",
    "\n",
    "\n",
    "> Up to a point, overfitting the model to the SFT dataset can continue to increase human preference ratings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Prompts\n",
    "\n",
    "Submit some prompts [here](https://docs.google.com/forms/d/e/1FAIpQLSfSBFzS4yrdUwy3DjEj2kskTc1JXk-T47TbmK8TaEgSt4fkcA/viewform?usp=sf_link).\n",
    "\n",
    "## Collecting Ideal Prompt Responses\n",
    "\n",
    "These days, there are more sophisticated ways to manage data collection.\n",
    "One platform is [Scale AI](https://scale.com).\n",
    "\n",
    "Submit your email [here](https://docs.google.com/forms/d/e/1FAIpQLScsMfW1Fh0bwget7cZKCzm6TQ-1c0AsvQFHtBain2l1mjnIcQ/viewform?usp=sf_link) so I can allow you to join my demo labelling workforce and you can access the Scale labelling platform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The best reasons to study at Oxford university:\\n- Full of culture\\n- Attracts great people\\n- Excellent staff', '- Every college has its own bar and drink')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "class SFTDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Supervised Fine-Tuning Dataset\n",
    "\n",
    "    Returns:\n",
    "        prompt: str\n",
    "        response: str\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        with open(\"raw_data.json\") as f:\n",
    "            self.data = json.load(f)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Defines the length of the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Defines how to get a sample from the dataset by indexing it.\n",
    "\n",
    "        Returns:\n",
    "            prompt: str\n",
    "            response: str\n",
    "        \"\"\"\n",
    "        return self.data[idx][\"prompt\"], self.data[idx][\"response\"]\n",
    "    \n",
    "\n",
    "dataset = SFTDataset()\n",
    "print(dataset[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, GPT-3.5 is not currently available to download. \n",
    "The model parameters are closed source and owned by OpenAI.\n",
    "So instead, we'll work with GPT-2 - a smaller predecessor of the model trained on the same task of language modelling. \n",
    "\n",
    "[GPT-2](https://huggingface.co/gpt2) (and original [GPT](https://huggingface.co/openai-gpt)) model are available through HuggingFace.\n",
    "\n",
    "Here are the key differences:\n",
    "- Size:\n",
    "    - GPT has 117M parameters\n",
    "    - GPT-2 has 1.5B parameters\n",
    "    - GPT-3 has 175B parameters (800GB storage required)\n",
    "    \n",
    "- Training data size: \n",
    "    <!-- - GPT: -->\n",
    "    - GPT-2: 40GB (8M webpages)\n",
    "    - GPT-3: 45TB\n",
    "<!-- - Training data variety:\n",
    "The GPT models were trained on increasingly larger and more diverse datasets, with GPT-3 trained on a massive corpus of web pages, books, and other text sources. -->\n",
    "- Task performance: GPT-3 has demonstrated better performance on a wide range of natural language tasks, including question answering, language translation, and natural language generation. It has also shown an ability to perform some common sense reasoning and to generate coherent and informative responses even to complex prompts.\n",
    "\n",
    "- Speed and efficiency: Because of its size, GPT-3 is slower and more resource-intensive to run than GPT-2 or the original GPT. However, it can generate high-quality outputs with fewer prompts or examples.\n",
    "\n",
    "- Release date: The original GPT was released in 2018, GPT-2 in 2019, and GPT-3 in 2020. Each new model represents a significant advance in natural language processing capabilities.\n",
    "\n",
    "Let's load in GPT-2 and make sure we're comfortable with how it can be used to generate new text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am a language model.\n",
      "\n",
      "To understand our problems, it must be explained how most languages do it.\n",
      "\n",
      "Language Models\n",
      "\n",
      "So how do we know languages are valid?\n",
      "\n",
      "Languages are all about the relationships between\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", config=configuration) # Load the tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\") # Load the model\n",
    "\n",
    "# generate a sequence of tokens using the model's forward method\n",
    "prompt = \"Hello, I am a language model.\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement a function to initiate a back and forth chat with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    \"\"\"Chat with the model.\"\"\"\n",
    "    prompt = \"\"\n",
    "    while True:\n",
    "        # GET USER INPUT\n",
    "        next_input = \"You: \" + input(\"You: \") + \"\\nBot: \"\n",
    "        print(next_input)\n",
    "        prompt += next_input\n",
    "\n",
    "        # GENERATE A SEQUENCE OF TOKENS USING THE MODEL'S FORWARD METHOD\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        output = model.generate(input_ids, max_length=100, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)\n",
    "\n",
    "        # PRINT THE RESPONSE AND UPDATE THE PROMPT\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        print(response)\n",
    "        prompt += response\n",
    "\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:02<00:06,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.04907530918717384 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:03<00:03,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.12487148493528366 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:05<00:01,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2410813570022583 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:07<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6012824177742004 3\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:01<00:04,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.044717010110616684 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:02<00:03,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.03734680637717247 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:04<00:01,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08754557371139526 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.44542935490608215 7\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.037838295102119446 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:03<00:03,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08416501432657242 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:05<00:01,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3369210362434387 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:06<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.05517173931002617 11\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:01<00:04,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08211680501699448 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:03<00:03,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.23072893917560577 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:05<00:01,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.04853944852948189 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:07<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.041240278631448746 15\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:02<00:07,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.14014741778373718 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:04<00:04,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.05555743724107742 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:06<00:01,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.07117579877376556 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:07<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.03610841929912567 19\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:01<00:05,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.05602844059467316 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:03<00:03,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.03551921993494034 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:05<00:01,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0998191386461258 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:07<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.07416810840368271 23\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:02<00:07,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.05246926471590996 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:04<00:04,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.036311373114585876 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:05<00:01,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06711934506893158 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:07<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.061407703906297684 27\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.05729812756180763 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:03<00:03,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.05340489372611046 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:05<00:01,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.055069807916879654 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:07<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.10297903418540955 31\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:01<00:04,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.04357868432998657 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:02<00:02,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06658864766359329 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:04<00:01,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.048337750136852264 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:06<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.054456938058137894 35\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:02<00:06,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.04976287856698036 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:03<00:03,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.03473042696714401 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:05<00:01,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.043598826974630356 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:06<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.07417843490839005 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# print(model.modules)\n",
    "# scsdc\n",
    "\n",
    "def train(epochs=10):\n",
    "    # Create the dataset and dataloader\n",
    "    dataset = SFTDataset()\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.95)) # as used in the InstructGPT paper\n",
    "\n",
    "    # Set up logging\n",
    "    writer = SummaryWriter() # for logging our loss to TensorBoard\n",
    "    batch_idx = 0 # for setting the x-axis of our TensorBoard plots (loss vs. batch index)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        for batch in tqdm(dataloader):\n",
    "            # Get the data\n",
    "            prompt, response = batch\n",
    "            prompt = prompt[0]\n",
    "            response = response[0]\n",
    "\n",
    "            # Encode the data\n",
    "            entire_text = prompt + response\n",
    "            context_dict = tokenizer(\n",
    "                '<|startoftext|>' + entire_text + '<|endoftext|>',\n",
    "                                    #    truncation=True, \n",
    "                                    #    max_length=max_length, \n",
    "                                    #    padding=\"max_length\"\n",
    "            )\n",
    "\n",
    "            # input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "            # print(input_ids)\n",
    "            # print(tokenizer.decode(input_ids[0], skip_special_tokens=False))\n",
    "            # print(len(input_ids))\n",
    "            # labels = tokenizer.encode(response, return_tensors=\"pt\")\n",
    "            # print(tokenizer.decode(labels[0], skip_special_tokens=False))\n",
    "            # print(len(labels))\n",
    "            # sdcs\n",
    "\n",
    "            input_ids = torch.tensor(context_dict.input_ids)\n",
    "            labels = torch.tensor(context_dict.input_ids)\n",
    "            attention_mask = torch.tensor(context_dict.attention_mask)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                labels=labels,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "\n",
    "            # logits = outputs.logits\n",
    "\n",
    "\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Log the loss\n",
    "            print(f\"Loss: {loss.item()}\", batch_idx)\n",
    "            writer.add_scalar(\"Loss/train\", loss.item(), batch_idx)\n",
    "            batch_idx += 1\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
