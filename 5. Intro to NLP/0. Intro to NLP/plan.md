## What is a tokeniser?

## TODO diagram

## What is a vocab?

## TODO diagram

# TODO diagram of word vectors

### The problem with 1-hot encodings

#### Similar words do not have similar representations

- Similar words do not have similar representations
- all vectors are orthogonal

# TODO diagram

## Word embeddings

- "you shall know a word by the company it keeps"

## Word representations can be learnt in a number of ways

## word2vec

# TODO diagram

### They can be learnt for a specific problem

- E.g. including a randomly initialised embedding layer in a sentiment classification task

- The most common of these is BERT

## Pre-trained word embeddings

_get BERT model_

_get pre-trained BERT embeddings_

_visualise pre-trained BERT embeddings in tensorboard_

## Where are we on the map of our journey to ChatGPT?

- we now know how to represent text data numerically
