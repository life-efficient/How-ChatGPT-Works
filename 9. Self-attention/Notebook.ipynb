{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "\n",
    "## Motivation\n",
    "\n",
    "The original attention mechanism allowed the decoder to focus on different inputs to the encoder.\n",
    "\n",
    "Is there a way to make the attention mechanism of the **encoder** pay attention to the different tokens of **its own inputs**?\n",
    "\n",
    "\n",
    "The answer, is yes, and we call that _self-attention_.\n",
    "\n",
    "> In a self-attention layer, every input attends to every other input\n",
    "\n",
    "> The output of a self-attention layer is the same number of same sized vectors as the input\n",
    "\n",
    "> Each output is a weighted average of the inputs, weighted by how much \n",
    "\n",
    "> Self-attention can be applied in every layer, not just the first"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Combining neural attention with a RNN based sequence to sequence model gives you the model shown below, made up of three core building blocks:\n",
    "1. The encoder\n",
    "1. The decoder\n",
    "1. The cross-attention mechanism\n",
    "\n",
    "# TODO add outline to dec-enc blocks\n",
    "\n",
    "![](../images/RNN%20Seq2seq%20Attention.png)\n",
    "\n",
    "The goal now is to try to see if we can replace any of the building blocks in this architecture. \n",
    "\n",
    "## Problems with RNN encoders\n",
    "1. looking in sequential order is how we write text, but it’s not how we understand them. You may have the subject of a sentence “the man…” followed by something describing the man, separated by a very long sequence. It’s hard to persist the representation between the two ends so that they can interact to build the right encoding. RNNs bake in this order into their encoding. \n",
    "1. Long-distance dependencies are also hard to learn because of gradient problems. \n",
    "1. Lack of parallelisability. Future RNN states can’t be computed until all other preceding sequences. You have to do T steps of computation before you can make a gradient step. \n",
    "\n",
    "> Recurrence, despite how useful for encoding, is the cause of these problems. \n",
    "\n",
    "### Our goal now is to resolve the issue of time-dependency and long-term dependencies caused by RNNs\n",
    "\n",
    "## Tackling the time dependency\n",
    "\n",
    "The recurrent nature of RNNs means that the sequences can only be processed sequentially, which means that the time complexity of parameter update steps is $O(T)$. \n",
    "We need to find an alternative building block that can be used to encode/decode our sequences.\n",
    "\n",
    "One alternative, is to use _word windows_ - a \"window\" of fixed size applied \n",
    "\n",
    "# TODO diagram Show very useful diagram of how many steps required to get to each layer. \n",
    "\n",
    "Windows in every position across text can be computed immediately, in parallel, with no dependence in time. This tackles parallelisation, but not long-range dependencies. \n",
    "\n",
    "# TODO diagram Shows useful diagram of which neuron can influence each between layers (pyramid-looking). \n",
    "\n",
    "As you can see from this diagram, each neuron is a combination of only a few neurons in the layer below. \n",
    "\n",
    "## Building block 2: Attention\n",
    "\n",
    "Attention, in general treats word’s representation as a query to access and incorporate information from a set of values. \n",
    "\n",
    "In a RNN seq2seq model, the set of encoder states for the source sentence are the values, the decoder state was the query, and their dot product gave an attention score.\n",
    "\n",
    "x\n",
    "\n",
    "## Self-attention introduces another set of vectors, the _keys_, as well as queries and values found in attention\n",
    "\n",
    "Recall that attention operates on queries (q), keys (k) and values (v). \n",
    "# TODO put q, k, v in attention notebook\n",
    "\n",
    "In self-attention, $q$, $k$ and $v$ come from the same source (like the same sentence). \n",
    "Where do these come from? \n",
    "\n",
    "> Regardless of what form of attention you use, and what $q$, $k$, and $v$ are, you’re doing the same thing: dot product of queries and keys to get the “affinities” (alignment), then creating a affinity-weighted combination of the input values.\n",
    "\n",
    "How is this different from a fully connected layer now that you’re connecting eveything to everything? \n",
    "1. Dynamic connectivity: The connection weights vary as a function of the input, because they are computed from the affinity between the keys and queries. In a neural network, the connections between each layer are the same for every input. Transformers learn the alignment function which determines the connections between layers for each example.\n",
    "2. The parameterisation is very different. “It has this inductive bias that’s not just everything to everything feedforward”. \n",
    "\n",
    "You get a key, query, value for each word embedding. \n",
    "You can stack the self-attention layers and have k, q, v at each layer. \n",
    "\n",
    "## SELF-ATTENTION as described so far CANNOT yet be used as a building block. \n",
    "\n",
    "There are several problems which we need to address:\n",
    "\n",
    "### Problem 1: Positional Encoding\n",
    "The order of words obviously matters, but the sliding window approach currently contains no information about where each word appears. So we need to encode this. So far, it is an operation on sets rather than an operation on an ordered sequence.\n",
    "\n",
    "Let’s bound? the sentence length as T. \n",
    "For each i \\in {1, …, T} get a positional encoding p_i. \n",
    "Then just add that to each of the self-attention block inputs (q,k,v). \n",
    "Simple way to add this would be to just get q = v_tilde + p_i. You could concat them, but simple and common to just add. \n",
    "\n",
    "You can do the sinusoid thing to get positional encodings, which gives you pros: \n",
    "- periodicity indicates that absolute position is not as important\n",
    "- Maybe can extrapolate to longer sequences\n",
    "and cons: \n",
    "- It's not learnable - perhaps a better positional encoding could be learnt?\n",
    "- Extrapolation doesn’t really work\n",
    "\n",
    "More commonly nowadays is to learn the $p_i$. \n",
    "Set a $d x T$ (size by seq len) matrix $P$. \n",
    "\n",
    "Pros:\n",
    "- Flexible: each position gets to be learned to fit the data.\n",
    "\n",
    "Cons:\n",
    "- You can’t extrapolate to sequences longer than $T$ because you haven’t learnt how to represent them. \n",
    "\n",
    "Other ways to encode $P$ include relative position between words of position representations that depend on syntax. \n",
    "\n",
    "# TODO diagram of positional encoding\n",
    "\n",
    "### Problem 2: No Nonlinearities\n",
    "There are no nonlinearities, so the sequential self-attentions just average averages rather than building hierarchically. \n",
    "\n",
    "Solution: add a feedforward layer between self-attention blocks. \n",
    "\n",
    "Intuition is that the feedforward layers “process the result of the attention”. \n",
    "\n",
    "# TODO improve this section\n",
    "\n",
    "### Problem 3: Future Tokens in the Decoder Should be Hidden\n",
    "Self-attention looks at the whole sequence at once, which is cheating for language modelling! It’s ok for that to happen in an encoder, but not in a decoder. So we mask the future in self-attention. \n",
    "\n",
    "One solution would be change the keys and values each timestep, but that would be inefficient. Instead, just set the attention affinities to $-inf$, which makes the attention weights 0.\n",
    "# TODO improve\n",
    "\n",
    "# TODO diagram\n",
    "\n",
    "### Having addressed these problems, self-attention can now be used as a building block in the seq2seq model\n",
    "\n",
    "As a recap:\n",
    "- We removed recurrence by applying a sliding window\n",
    "- We then introduced a positional encoding to the inputs to tell the model the position of each word\n",
    "- We added nonlinearities between each layer of self attention to allow it to build hierarchical representations\n",
    "- We apply masking to any decoder self-attention inputs to ensure that the model can't \"cheat\" and see the future of tokens which during evaluation/inference would not be visible"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Attention\n",
    "When we implement self-attention on the decoder, it should not see the future tokens.\n",
    "\n",
    "This could be done using a for loop, but it is far more efficient to vectorise this operation.\n",
    "\n",
    "We can implement that by setting their weights to zero.\n",
    "\n",
    "That can be done by setting their affinities to negative inifinity. When they are normalised\n",
    " through the softmax function, they will become zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ice/opt/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def mask_hidden_tokens(attention_affinities):\n",
    "    T = attention_affinities.shape[0]\n",
    "    mask = torch.tril((torch.ones(T, T))) # mask showing which tokens should have access to others\n",
    "    return attention_affinities.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "T = 4\n",
    "attention_affinities = torch.ones((T, T))\n",
    "\n",
    "print(attention_affinities)\n",
    "\n",
    "attention_affinities = mask_hidden_tokens(attention_affinities)\n",
    "attention_weights = F.softmax(attention_affinities, dim=-1)\n",
    "\n",
    "print(attention_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, demo affinities between each token are set equal to one.\n",
    "\n",
    "In the complete masked attention module, the affinities between keys and queries would be computed differently, depending on the type of attention implemented. \n",
    "E.g. by taking the dot product between keys and queries in dot product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "input_token_embeddings = torch.zeros((B, T, C))\n",
    "# simplest way to inject prev context is to average - TODO use RNN or transformer\n",
    "\n",
    "\n",
    "attention_weights = torch.zeros((T, T)) # these are the attention weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weighted_masked_inputs = input_attention_embeddings @ attention_weights\n",
    "\n",
    "\n",
    "class LanguageModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        vocab_size = 100\n",
    "        embedding_dim = 128\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding_embeddings = torch.nn.Embedding(sequence_length, embedding_dim)\n",
    "        self.head = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        token_embeddings = self.embedding()\n",
    "        positional_encodings = self.positional_encoding_embeddings(torch.arange(T)) # T x D # this implementation limits the input length\n",
    "        input_token_embeddings = token_embeddings + positional_encodings\n",
    "        logits = self.head(input_token_embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query: What am I looking for?\n",
    "Key: What do I contain?\n",
    "\n",
    "Affinities = f(Q, K)\n",
    "\n",
    "All queries dot product with all keys to produce affinities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(torch.nn.Module):\n",
    "    def __init__(self, embedding_size, head_size=16):\n",
    "        super().__init__()\n",
    "        self.key = torch.nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.query = torch.nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.value = torch.nn.Linear(embedding_size, head_size, bias=False) # no bias we just want vectors, not linear transforms\n",
    "        # if we want the mask to be stored as part of the state of the model (but not as a parameter), then we should use self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # GET KEYS, VALUES AND QUERIES FOR THIS INPUT\n",
    "        keys = self.key(X) # what do I contain?\n",
    "        queries = self.query(X) # what am I looking for?\n",
    "        values = self.value(X)\n",
    "        \n",
    "        # COMPUTE ATTENTION WEIGHTS\n",
    "        keys = keys.transpose(-2, -1) # transpose T & D dimensions so that keys are \n",
    "        attention_affinities = queries @ keys \n",
    "        attention_affinities = mask_hidden_tokens(attention_affinities)\n",
    "        attention_affinities /= self.embedding_size**0.5 # normalise by sequence size # this makes it \"scaled dot product attention\"\n",
    "        attention_weights = F.softmax(attention_affinities)\n",
    "\n",
    "        # COMPUTE ATTENTION WEIGHTED CONTEXT\n",
    "        context = attention_weights @ values\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLanguageModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, sequence_length):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.positional_encoding_table = torch.nn.Embedding(sequence_length, embedding_size)\n",
    "        self.self_attention_head = SelfAttentionHead(embedding_size)\n",
    "        self.head = torch.nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, X, targets=None):\n",
    "        # X is (B, L)\n",
    "        embeddings = self.token_embedding_table(X)\n",
    "        positional_encodings = self.positional_encoding_table(X)\n",
    "        final_input_embeddings = embeddings + positional_encodings\n",
    "        context = self.self_attention_head(final_input_embeddings)\n",
    "        logits = self.head(context)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate():\n",
    "        # crop box size\n",
    "        pass \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Headed Self-Attention\n",
    "\n",
    "The key, query, value projections allow the model to learn how to embed each input token and attend to others depending on what tokens appear. \n",
    "But they can only learn to do that in one way.\n",
    "For any input, it's keys queries and values will only be represented in one way - the way defined by the parameters of that single self-attention head.\n",
    "\n",
    "It is possible that learning many different ways to represent the same inputs may be useful.\n",
    "\n",
    "So, what if we used multiple different self-attention heads in parallel? We call this multi-headed self-attention.\n",
    "\n",
    "To avoid multiplying the number of parameters used in the self-attention layers, we scale down the size of their embeddings by the number of heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, head_size=16, num_heads=4, embedding_size=32):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList(\n",
    "            [SelfAttentionHead(embedding_size, head_size) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.cat([head(X) for head in self.heads], dim=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put multi-head self-attention into our language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionLanguageModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, sequence_length, num_heads=4, embedding_size=32):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.positional_encoding_table = torch.nn.Embedding(sequence_length, embedding_size)\n",
    "        self.self_attention_head = MultiHeadSelfAttention(num_heads=num_heads, embedding_size=embedding_size // num_heads) # TODO add multi-head self-attention module\n",
    "        self.head = torch.nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, X, targets=None):\n",
    "        # X is (B, L)\n",
    "        embeddings = self.token_embedding_table(X)\n",
    "        positional_encodings = self.positional_encoding_table(X)\n",
    "        final_input_embeddings = embeddings + positional_encodings\n",
    "        context = self.self_attention_head(final_input_embeddings)\n",
    "        logits = self.head(context)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate():\n",
    "        # crop box size\n",
    "        pass \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO diagram directed graph of indexed nodes, where each node is pointed to by only itself and those before it\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffef1fb3247e42ae9cf3614f3519d4998b3b95643236a5d32641564963f5a3b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
