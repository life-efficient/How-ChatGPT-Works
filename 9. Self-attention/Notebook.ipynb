{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "\n",
    "Combining neural attention with a RNN based sequence to sequence model gives you the model shown below\n",
    "\n",
    "![](../images/RNN%20Seq2seq%20Attention.png)\n",
    "\n",
    "The goal now is to try to see if we can replace any of the building blocks in this architecture. \n",
    "\n",
    "## Problems with RNN encoders\n",
    "looking in sequential order is how we write text, but it’s not how we understand them. You may have the subject of a sentence “the man…” followed by something describing the man, separated by a very long sequence. It’s hard to persist the representation between the two ends so that they can interact to build the right encoding. RNNs bake in this order into their encoding. \n",
    "\n",
    "Long-distance dependencies are also hard to learn because of gradient problems. Lack of parallelisability. \n",
    "\n",
    "Future RNN states can’t be computed before all other past sequences. You have to do T steps of computation before you can make a gradient step. \n",
    "\n",
    "> Recurrence, despite how useful for encoding, is the cause of these problems. \n",
    "\n",
    "### Our goal now is to resolve the issue of time-dependency and long-term dependencies caused by RNNs\n",
    "\n",
    "## Tackling the time dependency\n",
    "\n",
    "The recurrent nature of RNNs means that the sequences can only be processed sequentially, which means that the time complexity of parameter update steps is $O(T)$. \n",
    "We need to find an alternative building block that can be used to encode/decode our sequences.\n",
    "\n",
    "One alternative, is to use _word windows_ - a \"window\" of fixed size applied \n",
    "\n",
    "# TODO diagram Show very useful diagram of how many steps required to get to each layer. \n",
    "\n",
    "Windows in every position across text can be computed immediately, in parallel, with no dependence in time. This tackles parallelisation, but not long-range dependencies. \n",
    "\n",
    "# TODO diagram Shows useful diagram of which neuron can influence each between layers (pyramid-looking). \n",
    "\n",
    "As you can see from this diagram, each neuron is a combination of only a few neurons in the layer below. \n",
    "\n",
    "## Building block 2: Attention\n",
    "\n",
    "Attention, in general treats word’s representation as a query to access and incorporate information from a set of values. \n",
    "\n",
    "In a RNN seq2seq model, the set of encoder states for the source sentence are the values, the decoder state was the query, and their dot product gave an attention score.\n",
    "\n",
    "The original attention mechanism allowed the decoder to focus on different inputs to the encoder.\n",
    "\n",
    "Several questions follow?\n",
    "1. Is there a way to make the attention mechanism of the **encoder** pay attention to the different tokens of **its own inputs**?\n",
    "1. Even better, could it pay attention to different inputs of each layer throughout the model rather than just paying attention to the raw inputs?\n",
    "\n",
    "The answer to both, is yes, and we call that _self-attention_\n",
    "\n",
    "In self-attention, all hidden states can attend to all words even in the first layer after the embedding. \n",
    "\n",
    "Recall that attention operates on queries (q), keys (k) and values (v). \n",
    "# TODO put q, k, v in attention notebook\n",
    "\n",
    "In self-attention, $q$, $k$ and $v$ come from the same source (like the same sentence). \n",
    "Where do these come from? \n",
    "\n",
    "> Regardless of what form of attention you use, and what $q$, $k$, and $v$ are, you’re doing the same thing: dot product of queries and keys to get the “affinities” (alignment), then creating a affinity-weighted combination of the input values.\n",
    "\n",
    "How is this different from a fully connected layer now that you’re connecting eveything to everything? \n",
    "1. Dynamic connectivity: The connection weights vary as a function of the input, because they are computed from the affinity between the keys and queries. In a neural network, the connections between each layer are the same for every input. Transformers learn the alignment function which determines the connections between layers for each example.\n",
    "2. The parameterisation is very different. “It has this inductive bias that’s not just everything to everything feedforward”. \n",
    "\n",
    "You get a key, query, value for each word embedding. \n",
    "You can stack the self-attention layers and have k, q, v at each layer. \n",
    "\n",
    "## SELF-ATTENTION as described so far CANNOT yet be used as a building block. \n",
    "\n",
    "There are several problems which we need to address:\n",
    "\n",
    "### PROBLEM 1:\n",
    "The order of words obviously matters, but the sliding window approach currently contains no information about where each word appears. So we need to encode this. So far, it is an operation on sets rather than an operation on an ordered sequence.\n",
    "\n",
    "Let’s bound? the sentence length as T. \n",
    "For each i \\in {1, …, T} get a positional encoding p_i. \n",
    "Then just add that to each of the self-attention block inputs (q,k,v). \n",
    "Simple way to add this would be to just get q = v_tilde + p_i. You could concat them, but simple and common to just add. \n",
    "\n",
    "You can do the sinusoid thing to get positional encodings, which gives you pros: \n",
    "- periodicity indicates that absolute position is not as important\n",
    "- Maybe can extrapolate to longer sequences\n",
    "and cons: \n",
    "- It's not learnable - perhaps a better positional encoding could be learnt?\n",
    "- Extrapolation doesn’t really work\n",
    "\n",
    "More commonly nowadays is to learn the $p_i$. \n",
    "Set a $d x T$ (size by seq len) matrix $P$. \n",
    "\n",
    "Pros:\n",
    "- Flexible: each position gets to be learned to fit the data.\n",
    "\n",
    "Cons:\n",
    "- You can’t extrapolate to sequences longer than $T$ because you haven’t learnt how to represent them. \n",
    "\n",
    "Other ways to encode $P$ include relative position between words of position representations that depend on syntax. \n",
    "\n",
    "# TODO diagram of positional encoding\n",
    "\n",
    "### PROBLEM 2 with self-attention\n",
    "There are no nonlinearities, so the sequential self-attentions just average averages rather than building hierarchically. \n",
    "\n",
    "Solution: add a feedforward layer between self-attention blocks. \n",
    "\n",
    "Intuition is that the feedforward layers “process the result of the attention”. \n",
    "\n",
    "# TODO improve this section\n",
    "\n",
    "### PROBLEM 3 with self-attention\n",
    "Self-attention looks at the whole sequence at once, which is cheating for language modelling! It’s ok for that to happen in an encoder, but not in a decoder. So we mask the future in self-attention. \n",
    "\n",
    "One solution would be change the keys and values each timestep, but that would be inefficient. Instead, just set the attention affinities to $-inf$, which makes the attention weights 0.\n",
    "# TODO improve\n",
    "\n",
    "# TODO diagram\n",
    "\n",
    "### Having addressed these problems, self-attention can now be used as a building block in the seq2seq model\n",
    "\n",
    "As a recap:\n",
    "- We removed recurrence by applying a sliding window\n",
    "- We then introduced a positional encoding to the inputs to tell the model the position of each word\n",
    "- We added nonlinearities between each layer of self attention to allow it to build hierarchical representations\n",
    "- We apply masking to any decoder self-attention inputs to ensure that the model can't \"cheat\" and see the future of tokens which during evaluation/inference would not be visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (main, Nov 24 2022, 08:09:04) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3e643153f50b336c1c7a9d4d544c5113a86fd55c72312d55d3acd153a8b13ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
