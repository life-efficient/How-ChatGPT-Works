{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "At the heart of the Transformer is self-attention. Self-attention is a mechanism that allows each input in a sequence to look at the whole sequence to compute a representation of the sequence.\n",
    "\n",
    "...what?\n",
    "\n",
    "Ok. Let's look at the following sentence: `the animal didn't cross the street because it was too tired`. What does the `it` refer to in this sentence? The street or the animal? This is trivial for us as humans to answer but not for a machine. Wouldn't it be nice if we could have some way of the computer understanding what `it` referred to?\n",
    "\n",
    "This is what self-attention attempts to do. As the model processes each word in the input sequence, self-attention allows us to look at other words in the input sequence for ideas as to what we want to encode in the representation of this word.\n",
    "\n",
    "![](http://jalammar.github.io/images/t/transformer_self-attention_visualization.png)\n",
    "\n",
    "It is given by the formula:\n",
    "\n",
    "$$Attention(Q,K,V) = softmax \\left( \\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "\n",
    "To make this clearer, think of how a hidden state in an RNN incorporates the representation of the previous words into the current representation. Self-attention is how the Transformer attempts to use other words (not just the previous words) to encode the meaning of a particular word\n",
    "\n",
    "\n",
    "![encoder_multihead](images/encoder_multihead.png)\n",
    "\n",
    "![vector_attn_score](images/vector_attn_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention in detail\n",
    "\n",
    "Self-attention is a representation which you can think of as a score. The intent is to have a representation, per input word, which tells us much how much focus each word needs to pay to every word in the sequence. A bit complicated right? Hopefully by the end of this post, this meaning will be clear.\n",
    "\n",
    "Before talking about matrices, let's talk in terms of vectors. We'll have the sequence length be of dimension $T$, and the encoding/embeddings of the word be $D$ dimensional.\n",
    "\n",
    "Note that because I'm focusing on the FIRST Encoder here, the encodings of our sequence is the embeddings. But as you move up the encoder stack, the encoding of the sequence is the output from the previous encoder. Again, this is $D$ dimensional.\n",
    "\n",
    "![word_encodings](images/word_encodings.png)\n",
    "\n",
    "\n",
    "Ok. Now we're going to create three vectors for EACH input: A query vector ($q$), a key vector ($k$), and a value vector ($v$). These will be $d_k$ dimensional. $d_k$ is typically $D/8$. As far as real values go, usually $D=512$, while $d_k=64$. In our example, what is $d_k$?\n",
    "\n",
    "Ok... so how how do we get $q$, $k$, $v$? We learn it of course!\n",
    "\n",
    "how... do we learn it? We need a weights matrix which will transform our encodings into these vectors.\n",
    "This means that $W^Q$, $W^K$ and $W^V$ are all $\\in \\mathbb{R}^{D×d_k}$\n",
    "\n",
    "![qkv_vectors](images/qkv_vectors.png)\n",
    "\n",
    "Ok, that's nice. We understand that $q$, $k$, $v$ are different projections of the same input now; but what do the query, key, value abstractions actually mean? They're useful terms we can use to think about attention. Let's look through the following so we can see the roles they play.\n",
    "\n",
    "Recall what we defined self-attention as earlier: A representation, per input word, which tells us much how much focus each word needs to pay to every word in the sequence. For each word in our sequence, we will calculate a score by taking the dot product of the current word's query vector and the key vector for every word in the sequence.\n",
    "\n",
    "![w1_til_softmax](images/w1_til_softmax.png)\n",
    "\n",
    "Let's take stock of what we've done so far:\n",
    "- The $÷\\sqrt{d_k}$ is simply a practical scaling factor which leads to stabler gradients.\n",
    "- Softmax turns our scores into a probability distribution (each score is now between 0 and 1, and the sum of the scores = 1).\n",
    "\n",
    "We will now use these scores by multiplying them with their value vector. The intention here is that lower scoring words will have less weighting in the self-attention output as these words will now have a sense of \"irrelevantness\" (e.g. a low score like 0.0001 will \"cancel out\" its corresponding value vector).\n",
    "\n",
    "![w1_til_z1](images/w1_til_z1.png)\n",
    "\n",
    "Finally, the output for the current word is the summation of all the $softmax \\times v$ vectors. I.e. a weighted sum:\n",
    "\n",
    "![z_vector](images/z_vector.png)\n",
    "\n",
    "Ok. So that's self-attention in vector form. What about in terms of matrices?\n",
    "\n",
    "- Our input, $X$, is now a matrix of our sequence of words (i.e. $X \\in \\mathbb{R}^{T\\times D}$):\n",
    "![x_matrix](images/X_matrix_input.png)\n",
    "\n",
    "- $Q$, $K$, $V$ are now also matrices $\\in \\mathbb{R}^{T \\times d_k}$.\n",
    "- $W^Q$,$W^K$,$W^V$ stay $\\in \\mathbb{R}^{D \\times d_k}$.\n",
    "- We now simply obtain $Z \\in \\mathbb{R}^{T \\times d_k}$ by plugging $Q$, $K$, $V$ into our Attention formula.\n",
    "\n",
    "![Z_matrix](images/Z_matrix.png)\n",
    "\n",
    "\n",
    "- For the FIRST encoder, Q, K, V are determined by the embeddings of the input words\n",
    "- For the rest of the encoder stack, Q, K, V are determined by the output of the previous encoder\n",
    "- For the decoder stack, Q is determined in a similar fashion to the encoders. K and V, however, are passed from the final encoder to each of the decoders in the decoder stack."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (main, Nov 24 2022, 08:09:04) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3e643153f50b336c1c7a9d4d544c5113a86fd55c72312d55d3acd153a8b13ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
