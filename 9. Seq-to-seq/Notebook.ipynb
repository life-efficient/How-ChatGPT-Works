{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is to seq-to-seq modelling?\n",
    "\n",
    "Seq-to-seq (sequence-to-sequence) models are a type of neural network that can be used to map an input sequence to an output sequence. They are commonly used in natural language processing (NLP) tasks such as machine translation, language summarization, and question answering.\n",
    "\n",
    "Seq-to-seq models consist of two main components: an encoder and a decoder. The encoder processes the input sequence and produces a fixed-length context vector that captures the relevant information from the input. The decoder then uses the context vector to generate the output sequence.\n",
    "\n",
    "# TODO image.png\n",
    "\n",
    "The encoder and decoder can be implemented using any type of neural network, such as a fully-connected network, a convolutional network, or a recurrent neural network (RNN). RNNs are particularly well-suited for seq-to-seq modelling because they can handle variable-length sequences and capture temporal dependencies.\n",
    "\n",
    "Seq-to-seq models are trained using supervised learning, where the input and output sequences are paired. During training, the model is given an input sequence and the corresponding output sequence, and it learns to predict the output sequence given the input sequence.\n",
    "\n",
    "In this lesson, we will learn how to implement a basic seq-to-seq model using PyTorch. We will start by preprocessing the data and creating a dataset object to iterate through the input and output sequences. Then, we will define the encoder and decoder models and train the seq-to-seq model using an optimizer and a loss function. Finally, we will use the trained model to generate new output sequences given input sequences.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ice/opt/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using custom data configuration en-hr-lang1=en,lang2=hr\n",
      "Found cached dataset tatoeba (/Users/ice/.cache/huggingface/datasets/tatoeba/en-hr-lang1=en,lang2=hr/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n",
      "100%|██████████| 1/1 [00:00<00:00, 34.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '1298',\n",
       " 'translation': {'en': 'Welcome to Tatoeba!', 'hr': 'Dobrodošla na Tatoeba.'}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "dataset = load_dataset(\"tatoeba\", lang1=\"en\", lang2=\"hr\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset[random.randint(0, len(dataset))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "First, let's start by preprocessing the data. We will use a dataset of English-French translations for this example. We will need to convert the data to lowercase, tokenize it, and create a vocabulary of unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "source_tokeniser = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "target_tokeniser = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\", lang=\"hr\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at how they tokenise input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 8667, 1362, 102]\n"
     ]
    }
   ],
   "source": [
    "encoded_example = source_tokeniser.encode(\"Hello world\")\n",
    "print(encoded_example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are the tokenised sequences longer than the number of words in the sequence?\n",
    "\n",
    "Decode the sequence back into text to take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Hello world [SEP]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_tokeniser.decode(encoded_example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that some \"special tokens\" have been inserted around the sequence.\n",
    "\n",
    "- `[CLS]` represents the start of the sequence\n",
    "    - Its name comes from our tokeniser which is used to train BERT. During BERT's training process, the model is asked to perform some classification to better understand the text. The `[CLS]` token represents the start of a new sentence that BERT is asked to classify. It's not important to understand any more than that for now.\n",
    "    - In other tokenisers, this would be the equivalient of `[SOS]`, that more intuitively indicates the Start Of Sequence\n",
    "- `[SEP]` represents the end of the sequence\n",
    "    - Its name comes from our tokeniser which is used to train BERT. During BERT's training process, the model is asked to perform some classification to better understand the text. The `[SEP]` token represents the separation between sentences that BERT is asked to classify. It's not important to understand any more than that for now.\n",
    "    - In other tokenisers, this would be the equivalient of `[EOS]`, that more intuitively indicates the End Of Sequence\n",
    "\n",
    "Later, we will get around to generating new sequences (translations). To do that, we'll need to give the model the ids of the start of sequence token and the end of sequence token. Let's store those as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_start_of_sequence_token_id = target_tokeniser.get_vocab()[\"[CLS]\"]\n",
    "decoder_end_of_sequence_token_id = target_tokeniser.get_vocab()[\"[SEP]\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset object\n",
    "\n",
    "Now that we have preprocessed the data, let's create a dataset object to iterate through the input and output sequences. We will use a PyTorch Dataset object for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-hr-lang1=en,lang2=hr\n",
      "Found cached dataset tatoeba (/Users/ice/.cache/huggingface/datasets/tatoeba/en-hr-lang1=en,lang2=hr/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n",
      "100%|██████████| 1/1 [00:00<00:00, 170.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 146, 1138, 1106, 1301, 1106, 2946, 119, 102], [101, 46052, 10147, 177, 14477, 32650, 38573, 10116, 119, 102])\n",
      "[CLS] I have to go to sleep. [SEP]\n",
      "[CLS] Moram ići spavati. [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, source_lang=\"en\", target_lang=\"hr\"):\n",
    "        dataset = load_dataset(\"tatoeba\", lang1=source_lang, lang2=target_lang)\n",
    "        dataset = dataset[\"train\"]\n",
    "        self.examples = []\n",
    "        for ex in dataset:\n",
    "            ex = ex[\"translation\"]\n",
    "            \n",
    "            source_seq = ex[source_lang]\n",
    "            source_seq = source_tokeniser(source_seq)\n",
    "            source_seq = source_seq[\"input_ids\"]\n",
    "\n",
    "            target_seq = ex[target_lang]\n",
    "            target_seq = target_tokeniser(target_seq)\n",
    "            target_seq = target_seq[\"input_ids\"]\n",
    "            self.examples.append((source_seq, target_seq))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        source_seq, target_seq = example\n",
    "        return source_seq, target_seq\n",
    "\n",
    "\n",
    "def test_dataset():\n",
    "    dataset = TranslationDataset()\n",
    "    for example in dataset:\n",
    "        print(example)\n",
    "        source, target = example\n",
    "        print(source_tokeniser.decode(source))\n",
    "        print(target_tokeniser.decode(target))\n",
    "        print()\n",
    "        break\n",
    "\n",
    "\n",
    "test_dataset()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-hr-lang1=en,lang2=hr\n",
      "Found cached dataset tatoeba (/Users/ice/.cache/huggingface/datasets/tatoeba/en-hr-lang1=en,lang2=hr/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n",
      "100%|██████████| 1/1 [00:00<00:00, 255.11it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3z/29w5rr9d0k3_p863hm40sdnc0000gn/T/ipykernel_77365/342532346.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtest_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/3z/29w5rr9d0k3_p863hm40sdnc0000gn/T/ipykernel_77365/342532346.py\u001b[0m in \u001b[0;36mtest_dataloaders\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mloader_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{loader_name} loader example:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0melem_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0melem_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# It may be accessed twice, so we use a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def get_dataloaders(batch_size=2):\n",
    "    dataset = TranslationDataset()\n",
    "    train_len = round(0.8*len(dataset))\n",
    "    val_len = round(0.1*len(dataset))\n",
    "    test_len = len(dataset) - val_len - train_len\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_len, val_len, test_len])\n",
    "\n",
    "    batch_size = 2\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def test_dataloaders():\n",
    "    train_loader, val_loader, test_loader = get_dataloaders()\n",
    "    for loader_name, loader in [(\"Train\", train_loader), (\"Validation\", val_loader), (\"Test\", test_loader)]:\n",
    "        for example in loader:\n",
    "            print(f\"{loader_name} loader example:\")\n",
    "            features, labels = example\n",
    "            print(\"Features\")\n",
    "            print(features)\n",
    "            print(\"Features shape:\", features.shape)\n",
    "            print(\"Labels\")\n",
    "            print(labels)\n",
    "            print(\"Label shape:\", labels.shape)\n",
    "            print()\n",
    "            break\n",
    "    \n",
    "test_dataloaders()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable length of sequences causes an error because the (source or target) sequences in a batch need to be the same length. \n",
    "That's because mathematically, tensors can't have empty values. \n",
    "We need to pad each sequence full of `[PAD]` tokens so that they have the length of the longest one.\n",
    "\n",
    "Dataloaders use their `collate_fn` to group together examples returned from your dataset. It can be set using a keyword argument upon initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-hr-lang1=en,lang2=hr\n",
      "Found cached dataset tatoeba (/Users/ice/.cache/huggingface/datasets/tatoeba/en-hr-lang1=en,lang2=hr/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n",
      "100%|██████████| 1/1 [00:00<00:00, 374.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader example:\n",
      "Features\n",
      "tensor([[ 101, 2627, 1132, 1128,  136,  102,    0,    0],\n",
      "        [ 101, 2268, 1110, 1226, 1104, 2731,  119,  102]])\n",
      "Features shape: torch.Size([2, 8])\n",
      "Labels\n",
      "tensor([[  101, 30186, 10294, 14382,   136,   102,     0,     0,     0,     0],\n",
      "        [  101,   294, 11024, 34300, 10144, 14283, 12377, 50173,   119,   102]])\n",
      "Label shape: torch.Size([2, 10])\n",
      "\n",
      "Validation loader example:\n",
      "Features\n",
      "tensor([[  101,  1135,  1110, 21321,  1106, 24530,  1283,  1103,  2377,  1297,\n",
      "           119,   102],\n",
      "        [  101,  1188,  1110,   170,  3415,   119,   102,     0,     0,     0,\n",
      "             0,     0]])\n",
      "Features shape: torch.Size([2, 12])\n",
      "Labels\n",
      "tensor([[  101, 14321, 62019, 42176, 10343, 10144, 17674, 15847, 76014, 10325,\n",
      "         10339, 19434, 13501, 17165, 21826,   119,   102],\n",
      "        [  101, 11469, 10144, 19157,   119,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]])\n",
      "Label shape: torch.Size([2, 17])\n",
      "\n",
      "Test loader example:\n",
      "Features\n",
      "tensor([[  101,   146,  1354,  1115,  1108,  1139,  2261,   119,   102,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1247,   112,   188,  1185,  2554,  1142,  3653,  1110,  2416,\n",
      "          1118, 10211,   119,   102]])\n",
      "Features shape: torch.Size([2, 14])\n",
      "Labels\n",
      "tensor([[  101, 99946, 81523, 21083, 10143, 10144, 10114, 46912, 10418, 39327,\n",
      "         14875,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101, 34817, 10113, 17674, 29797, 10143, 10144, 58530, 14038, 13051,\n",
      "         13530, 41603, 87981, 10113, 28667, 10692, 12229, 36274, 10238,   119,\n",
      "           102]])\n",
      "Label shape: torch.Size([2, 21])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_dataloaders(batch_size=2):\n",
    "    dataset = TranslationDataset()\n",
    "    train_len = round(0.8*len(dataset))\n",
    "    val_len = round(0.1*len(dataset))\n",
    "    test_len = len(dataset) - val_len - train_len\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_len, val_len, test_len])\n",
    "\n",
    "    def collate_fn(batch):    \n",
    "        source = torch.nn.utils.rnn.pad_sequence([torch.tensor(ex[0]) for ex in batch], batch_first=True)\n",
    "        target = torch.nn.utils.rnn.pad_sequence([torch.tensor(ex[1]) for ex in batch], batch_first=True)\n",
    "        return source, target\n",
    "\n",
    "    batch_size = 2\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "test_dataloaders()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-hr-lang1=en,lang2=hr\n",
      "Found cached dataset tatoeba (/Users/ice/.cache/huggingface/datasets/tatoeba/en-hr-lang1=en,lang2=hr/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n",
      "100%|██████████| 1/1 [00:00<00:00, 219.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden shape: torch.Size([3, 2, 128])\n"
     ]
    }
   ],
   "source": [
    "# Define the encoder model\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, source_vocab_size, hidden_size=128, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = torch.nn.Embedding(source_vocab_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.init_hidden(len(input))\n",
    "        embedded = self.embedding(input)\n",
    "        output, self.hidden = self.gru(embedded, self.hidden)\n",
    "        return self.hidden # return the hidden state\n",
    "      \n",
    "    def init_hidden(self, batch_size):\n",
    "        self.hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "\n",
    "def test_encoder():\n",
    "    source_vocab_size = len(source_tokeniser.get_vocab())\n",
    "    target_vocab_size = len(target_tokeniser.get_vocab())\n",
    "    encoder = Encoder(source_vocab_size)\n",
    "    train_loader, _, _ = get_dataloaders()\n",
    "    for batch in train_loader:\n",
    "        inputs, _ = batch\n",
    "        encoder.init_hidden(len(inputs))\n",
    "        hidden = encoder(inputs)\n",
    "        break\n",
    "    print(\"Hidden shape:\", hidden.shape)\n",
    "\n",
    "test_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Decoding\n",
    "\n",
    "If target sequences have different lengths, then decoding them in a batch can be tricky or inefficient. It can be tricky because \n",
    "\n",
    "For simplicity, in this example, we will implement a decoder that decodes each sequence independently, instead of as part of a batch. That's going to require quite a bit more code compared to the encoder.\n",
    "\n",
    "### Decoder training vs inference\n",
    "\n",
    "When the model is being used to make predictions in the real world, the next prediction will have to continue from the previously predicted token - we have no labels in the wild!\n",
    "\n",
    "However, this can make training very difficult. That's because if a model predicts the incorrect token during decoding, and then bases the next token prediction upon that incorrect token, it's going to make it very hard to predict the correct token. As the sequence length increases, this problem gets worse. The previously incorrect tokens make it highly unlikely for the model to get anywhere close.\n",
    "\n",
    "To combat this, we can use _teacher forcing_ during training, which is where we disregard the previous predicted token, and instead pass the correct token from the labels to the model at the next timestamp.\n",
    "\n",
    "### `model.eval()` and `model.train()`\n",
    "\n",
    "These methods toggle the behaviour of child modules of a model that differ between training and evaluation.\n",
    "\n",
    "The do this by switching the `training` attribute of any `torch.nn.Module` subclass between `True` and `False`.\n",
    "\n",
    "### Where is the batch dimension needed?\n",
    "\n",
    "Recurrent PyTorch layers (`RNN`, `LSTM`, `GRU`) can process batched or unbatched examples. \n",
    "\n",
    "If the hidden state is found to be 2D ($N$ x $D$), then the model assumes that inputs are unbatched, and that they should have size $T$ x $D$.\n",
    "\n",
    "### Where is the time dimension needed?\n",
    "\n",
    "The time dimension is always expected by recurrent layers. This is true even if you're passing in individual timesteps (like we will implement below), in which case the size of that dimension should just be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-hr-lang1=en,lang2=hr\n",
      "Found cached dataset tatoeba (/Users/ice/.cache/huggingface/datasets/tatoeba/en-hr-lang1=en,lang2=hr/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n",
      "100%|██████████| 1/1 [00:00<00:00, 219.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mode tests passed\n",
      "True\n",
      "False\n",
      "Testing mode tests passed\n",
      "Hidden shape: torch.Size([3, 2, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, target_vocab_size, hidden_size, num_layers, start_of_sequence_token_id, end_of_sequence_token_id):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.start_of_sequence_token_id = start_of_sequence_token_id\n",
    "        self.end_of_sequence_token_id = end_of_sequence_token_id\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(target_vocab_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, num_layers, batch_first=False, bidirectional=False)\n",
    "        # TODO should this be batch first or not?\n",
    "        self.out = torch.nn.Linear(hidden_size, target_vocab_size)\n",
    "    \n",
    "    def forward(self, encodings, target_seqs=None):\n",
    "\n",
    "        if self.training:\n",
    "            assert target_seqs != None, \"The decoder requires targets in training mode to implement teacher forcing.\"\n",
    "            total_loss = 0\n",
    "        else:\n",
    "            assert target_seqs == None, \"The decoder should not receive targets in evaluation mode.\"\n",
    "\n",
    "        batch_size = encodings.shape[1]\n",
    "        decodings = []\n",
    "        for example_idx in range(batch_size):\n",
    "\n",
    "            encoding = encodings[:, example_idx, :]\n",
    "            \n",
    "            if self.training:\n",
    "                target_seq = target_seqs[example_idx]\n",
    "                decoding, loss = self.forward_single_example(encoding, target_seq)\n",
    "                total_loss += loss\n",
    "            else:\n",
    "                decoding = self.forward_single_example(encoding)\n",
    "            decodings.append(decoding)\n",
    "\n",
    "        if self.training:\n",
    "            return decodings, loss\n",
    "        else:\n",
    "            return decodings\n",
    "\n",
    "    # def eval(self):\n",
    "    #     super().eval() # do everything that the parent model would do\n",
    "    #     self.training = False\n",
    "    \n",
    "    # def train(self):\n",
    "    #     super().train()\n",
    "    #     self.training = True\n",
    "\n",
    "    def forward_single_example(self, encoding, target_seq=None):\n",
    "\n",
    "        if self.training:\n",
    "            assert target_seq != None, \"The decoder requires targets in training mode to implement teacher forcing.\"\n",
    "        else:\n",
    "            assert target_seq == None, \"The decoder should not receive targets in evaluation mode.\"\n",
    "\n",
    "        current_token_id = self.start_of_sequence_token_id\n",
    "        current_token_id = torch.tensor(current_token_id)\n",
    "        current_token_id = current_token_id.unsqueeze(0)\n",
    "        # embedding = embedding.unsqueeze(0) # add expected time dimension\n",
    "\n",
    "        # encoding = encoding.unsqueeze(1)\n",
    "        self.hidden = encoding\n",
    "\n",
    "        if self.training:\n",
    "            loss = 0\n",
    "\n",
    "        predicted_sequence = []\n",
    "        while True:\n",
    "\n",
    "            # Predict next token\n",
    "            embedding = self.embedding(current_token_id)\n",
    "            output, self.hidden = self.gru(embedding, self.hidden)\n",
    "            output = self.out(output)\n",
    "\n",
    "            # Calculate loss if in training mode\n",
    "            if self.training:\n",
    "                target_token_id = target_seq[0]\n",
    "                target_seq = target_seq[1:]\n",
    "                # print(output.shape)\n",
    "                # print(target_token_id.shape)\n",
    "                target_token_id = target_token_id.unsqueeze(0)\n",
    "                loss += F.cross_entropy(output, target_token_id)\n",
    "\n",
    "            current_token_id = torch.argmax(output, dim=1)\n",
    "            predicted_sequence.append(current_token_id.item())\n",
    "\n",
    "            # Implement teacher forcing if in training mode\n",
    "            if self.training:\n",
    "                current_token_id = target_token_id\n",
    "\n",
    "            # Stopping conditions\n",
    "            if len(predicted_sequence) > 10:\n",
    "                break\n",
    "            elif current_token_id == self.end_of_sequence_token_id:\n",
    "                break\n",
    "        if self.training:\n",
    "            # loss /= len(target_seq) # Normalise by sequence length\n",
    "            return predicted_sequence, loss\n",
    "        else:\n",
    "            return predicted_sequence\n",
    "    \n",
    "    # def init_hidden(self):\n",
    "    #     self.hidden = torch.zeros(self.num_layers, self.hidden_size) # Does not need batch dimension as it will process unbatched examples\n",
    "\n",
    "def test_decoder():\n",
    "    source_vocab_size = len(source_tokeniser.get_vocab())\n",
    "    target_vocab_size = len(target_tokeniser.get_vocab())\n",
    "    hidden_size = 128\n",
    "    num_layers = 3\n",
    "    encoder = Encoder(source_vocab_size, hidden_size, num_layers)\n",
    "    decoder = Decoder(target_vocab_size, hidden_size, num_layers, decoder_start_of_sequence_token_id, decoder_end_of_sequence_token_id)\n",
    "\n",
    "    max_target_seq_len = 20\n",
    "\n",
    "    train_loader, _, _ = get_dataloaders()\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        encoder.init_hidden(len(inputs))\n",
    "        hidden = encoder(inputs)\n",
    "\n",
    "        # Test training mode\n",
    "        decoder.train()\n",
    "        predicted_seq, loss = decoder(hidden, targets)\n",
    "        print(\"Training mode tests passed\")\n",
    "\n",
    "        # Test evaluation mode\n",
    "        print(decoder.training)\n",
    "        decoder.eval()\n",
    "        print(decoder.training)\n",
    "        predicted_seq = decoder(hidden)\n",
    "        print(\"Testing mode tests passed\")\n",
    "\n",
    "        # for example_idx in range(len(inputs)):\n",
    "        #     decoder.hidden = hidden[:, example_idx, :].unsqueeze(1)\n",
    "\n",
    "        #     current_token_id = target_tokeniser.get_vocab()[\"[CLS]\"]\n",
    "\n",
    "        #     predicted_sequence = [current_token_id]\n",
    "        #     for idx in range(max_target_seq_len):\n",
    "        #         current_token_id = torch.tensor(current_token_id).view(1, 1, 1)\n",
    "        #         prediction = decoder(current_token_id)\n",
    "        #         current_token_id = torch.argmax(prediction, dim=2)\n",
    "        #         predicted_sequence.append(current_token_id.item())\n",
    "        #         if current_token_id == target_tokeniser.get_vocab()[\"[SEP]\"]:\n",
    "        #             break\n",
    "        #     print(predicted_sequence)\n",
    "        break\n",
    "    print(\"Hidden shape:\", hidden.shape)\n",
    "\n",
    "test_decoder()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq = Encoder + Decoder\n",
    "\n",
    "Now we need to combine the encoder and decoder into a single model that encodes the source sequence, then decodes it into a target sequence.\n",
    "\n",
    "This should be pretty simple. The Seq2Seq model simply needs to:\n",
    "- Implement an initialiser\n",
    "    - Initialise an encoder\n",
    "    - Initialise a decoder\n",
    "- Implement the forward pass where:\n",
    "    - The source sequence is passed through the encoder to produce an encoding\n",
    "    - The encoding is passed through the decoder to produce a prediction of the target sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-hr-lang1=en,lang2=hr\n",
      "Found cached dataset tatoeba (/Users/ice/.cache/huggingface/datasets/tatoeba/en-hr-lang1=en,lang2=hr/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n",
      "100%|██████████| 1/1 [00:00<00:00, 126.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially random sequences generated:\n",
      "jugu Reese Reese Reeseтовой 996 996 996 יחד יחד יחד\n",
      "impressivenkinnkinlığınlığın ऑ ऑ踊 Reese Reese ordo\n",
      "Seq2Seq model tests passed\n"
     ]
    }
   ],
   "source": [
    "class Seq2Seq(torch.nn.Module):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size, num_layers, hidden_size, decoder_start_of_sequence_token_id, decoder_end_of_sequence_token_id):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(source_vocab_size, hidden_size, num_layers)\n",
    "        self.decoder = Decoder(target_vocab_size, hidden_size, num_layers, decoder_start_of_sequence_token_id, decoder_end_of_sequence_token_id)\n",
    "        self.eval() # Default to evaluation mode\n",
    "\n",
    "    def forward(self, source_seqs, target_seqs=None):\n",
    "        batch_size = len(source_seqs) # TODO you could eliminate this by randomly sampling from the dataset instead of indexing\n",
    "        encoding = self.encoder(source_seqs)\n",
    "        if self.training:\n",
    "            assert target_seqs != None, \"The seq2seq model requires targets in training mode to implement teacher forcing.\"\n",
    "            target_seq, loss = self.decoder(encoding, target_seqs)\n",
    "            return target_seq, loss\n",
    "        else:\n",
    "            target_seq = self.decoder(encoding)\n",
    "            return target_seq\n",
    "\n",
    "def test_seq2seq_model():\n",
    "    source_vocab_size = len(source_tokeniser.get_vocab())\n",
    "    target_vocab_size = len(target_tokeniser.get_vocab())\n",
    "    hidden_size = 128\n",
    "    num_layers = 3\n",
    "    seq2seq = Seq2Seq(source_vocab_size, target_vocab_size, num_layers, hidden_size, decoder_start_of_sequence_token_id, decoder_end_of_sequence_token_id)\n",
    "    train_loader, _, _ = get_dataloaders()\n",
    "    for batch in train_loader:\n",
    "        source_seqs, target_seqs = batch\n",
    "        predicted_target_seqs = seq2seq(source_seqs)\n",
    "        print(\"Initially random sequences generated:\")\n",
    "        for seq in predicted_target_seqs:\n",
    "            print(target_tokeniser.decode(seq))\n",
    "        break\n",
    "    print(\"Seq2Seq model tests passed\")\n",
    "\n",
    "test_seq2seq_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-hr-lang1=en,lang2=hr\n",
      "Found cached dataset tatoeba (/Users/ice/.cache/huggingface/datasets/tatoeba/en-hr-lang1=en,lang2=hr/0.0.0/b3ea9c6bb2af47699c5fc0a155643f5a0da287c7095ea14824ee0a8afd74daf6)\n",
      "100%|██████████| 1/1 [00:00<00:00, 217.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3z/29w5rr9d0k3_p863hm40sdnc0000gn/T/ipykernel_47408/3919367619.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparam_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mtest_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/3z/29w5rr9d0k3_p863hm40sdnc0000gn/T/ipykernel_47408/3919367619.py\u001b[0m in \u001b[0;36mtest_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_start_of_sequence_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_end_of_sequence_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparam_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mtest_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/3z/29w5rr9d0k3_p863hm40sdnc0000gn/T/ipykernel_47408/3919367619.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, hparam_dict, lr, epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Do optimisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "source_vocab_size = len(source_tokeniser.get_vocab())\n",
    "target_vocab_size = len(target_tokeniser.get_vocab())\n",
    "\n",
    "def train(model, dataloader, hparam_dict, lr=0.01, epochs=1):\n",
    "    model.train()    \n",
    "    batch_idx = 1\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    running_avg = None\n",
    "\n",
    "    optimiser = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            source_seqs, target_seqs = batch\n",
    "            \n",
    "            # Forward pass\n",
    "            prediction, loss = model(source_seqs, target_seqs)\n",
    "\n",
    "            # Log loss\n",
    "\n",
    "            writer.add_scalar(\"Loss/Train\", loss.item(), batch_idx)\n",
    "            running_avg = running_avg + ((loss.item() - running_avg) / batch_idx) if running_avg != None else loss.item()\n",
    "            if batch_idx % 100 == 0:\n",
    "                writer.add_hparams(hparam_dict, {f\"hparam/{batch_idx}-step loss\": running_avg})\n",
    "\n",
    "            for source_seq, prediction_seq, target_seq in zip(source_seqs, prediction, target_seqs): \n",
    "                source_seq = source_tokeniser.decode(source_seq)\n",
    "                prediction = target_tokeniser.decode(prediction_seq)\n",
    "                target_seq = target_tokeniser.decode(target_seq)\n",
    "                writer.add_text(\n",
    "                    \"Text\",\n",
    "                    f\"\"\"\n",
    "                    Source:    {source_seq}\n",
    "                    Predicted: {prediction}\n",
    "                    Label:     {target_seq}\n",
    "                    \"\"\", \n",
    "                    batch_idx\n",
    "                )\n",
    "            batch_idx += 1\n",
    "            # print(\"Loss:\", loss.item())\n",
    "\n",
    "            # Do optimisation\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "def test_train():\n",
    "    source_vocab_size = len(source_tokeniser.get_vocab())\n",
    "    target_vocab_size = len(target_tokeniser.get_vocab())\n",
    "    hidden_size = 256\n",
    "    num_layers = 3\n",
    "    batch_size = 8\n",
    "    lr = 0.01\n",
    "\n",
    "    hparam_dict = {\n",
    "        \"hidden_size\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": lr\n",
    "    }\n",
    "\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(batch_size=batch_size)\n",
    "    model = Seq2Seq(source_vocab_size, target_vocab_size, num_layers, hidden_size, decoder_start_of_sequence_token_id, decoder_end_of_sequence_token_id)\n",
    "    \n",
    "    train(model, train_loader, hparam_dict)\n",
    "\n",
    "test_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffef1fb3247e42ae9cf3614f3519d4998b3b95643236a5d32641564963f5a3b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
