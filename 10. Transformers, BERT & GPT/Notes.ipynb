{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# What is a Transformer?\n",
    "\n",
    "## Strengths\n",
    "- Expressive\n",
    "- Optimisable\n",
    "- Parallelisable/Efficient\n",
    "\n",
    "# An Overview of How Transformers Work\n",
    "\n",
    "- Encoder-Decoder Models\n",
    "- Cross Attention\n",
    "- Self-Attention\n",
    "- Multi-Head Self-Attention\n",
    "\n",
    "## The encoder learns\n",
    "- What is this input data? How should it be represented? How do parts of the input change how other parts should be represented?\n",
    "\n",
    "# How Transformers Work Part 1: Positional Encoding\n",
    "\n",
    "# How Transformers Work Part 2: The Encoder\n",
    "\n",
    "# How Transformers Work Part 3: The Decoder\n",
    "\n",
    "\n",
    "# EXTRAS\n",
    "\n",
    "# Limitations of Recurrent Models like LSTMs & RNNs\n",
    "- Even bidirectional RNNs don't really "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
