{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# What is a Transformer?\n",
    "\n",
    "## Strengths\n",
    "- Expressive\n",
    "- Optimisable\n",
    "- Parallelisable/Efficient\n",
    "\n",
    "# An Overview of How Transformers Work\n",
    "\n",
    "- Encoder-Decoder Models\n",
    "- Cross Attention\n",
    "- Self-Attention\n",
    "- Multi-Head Self-Attention\n",
    "\n",
    "## The encoder learns\n",
    "- What is this input data? How should it be represented? How do parts of the input change how other parts should be represented?\n",
    "\n",
    "# How Transformers Work Part 1: Positional Encoding\n",
    "\n",
    "# How Transformers Work Part 2: The Encoder\n",
    "\n",
    "# How Transformers Work Part 3: The Decoder\n",
    "\n",
    "\n",
    "# EXTRAS\n",
    "\n",
    "# Limitations of Recurrent Models like LSTMs & RNNs\n",
    "- Even bidirectional RNNs don't really "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
