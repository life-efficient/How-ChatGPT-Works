# TODO image of transformer model performance compared to others

# TODO table of benchmark results populated with mostly transformers

# Various transformer configurations

- encoder models
- decoder models
- encoder-decoder models

## Implementing the transformer layers

## Pre-trained transformers in action

## Famous transformers

## BERT

- Why does the openAI API measure things in terms of tokens rather than words?
  - The model is based on a byte-pair encoding tokeniser

## GPT

### Generative pretrained transformer

- now we are getting somewhere close to ChatGPT...

## Where are we on the map of our journey to ChatGPT?
