{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks are General Function Approximators\n",
    "\n",
    "Part of the reason that neural networks are an incredible tool, is because they can learn to represent any continuous function.\n",
    "They are _general function approximators_.\n",
    "\n",
    "That means, that whatever your input-output relationship, a neural network can theoretically learn it.\n",
    "\n",
    "# TODO img\n",
    "\n",
    "In fact, a neural network with just one hidden layer, can represent _any_ continuous function, as long as that hidden layer has enough nodes. \n",
    "\n",
    "We refer to this as the _universal approximation theorem_.\n",
    "\n",
    "Caveats:\n",
    "- \"enough nodes\" can in practice mean \"an infinite number of nodes\"\n",
    "- The network can only represent the ideal function given the ideal parameterisation, and finding that ideal parameterisation (or one that's close enough) can be extremely challenging\n",
    "\n",
    "Universal approximation of neural networks can be demonstrated in a few steps:\n",
    "1. Acknowledge that any continuous function can be accurately approximated by a piece-wise function with enough pieces\n",
    "1. Write the full equation for a NN with ReLU activation\n",
    "1. Show that it is equivalent to a sum of ReLUs\n",
    "1. Show that the sum of ReLUs is equivalent to a piece-wise function\n",
    "\n",
    "## Step 1: Any continuous function can be approximated by a piece-wise function\n",
    "\n",
    "The shape of any function can be drawn roughly by connecting straight lines.\n",
    "\n",
    "The more lines that you connect together, the more accurately you can represent the function.\n",
    "\n",
    "With an infinite number of infinitely short lines, you can perfectly represent the function.\n",
    "\n",
    "# TODO img of different size lines approximating a graph\n",
    "\n",
    "## Step 2: The full equation for a NN\n",
    "\n",
    "The full equation of a ReLU activation function is given as follows:\n",
    "\n",
    "# TODO full eqn\n",
    "\n",
    "This is the function that we wish to show can approximate any continuous function.\n",
    "\n",
    "## Step 3: The single layer NN is equivalent to a sum of ReLUs\n",
    "\n",
    "If we expand the full equation, it can be rearranged as a sum of ReLUs\n",
    "\n",
    "## Step 4: A sum of ReLUs is equivalent to a piece-wise function\n",
    "\n",
    "If we plot all of the different ReLU terms on the same graph, you can see the difference between each of them caused by their different parameters.\n",
    "\n",
    "\n",
    "- The associated input weight stretches the x-dimension\n",
    "\n",
    "# TODO img\n",
    "\n",
    "- The associated output weight stretches the y-dimension\n",
    "\n",
    "# TODO img\n",
    "\n",
    "- The input bias shifts where the ReLU input becomes greater than zero. Beyond this value, the output is switched on.\n",
    "\n",
    "As per the NN equation, the network is a sum of all of these ReLUs. When we add them together, we can see how they approximate the input-output relationship we are trying to model:\n",
    "\n",
    "![](images/side-by-side%20approximation%20and%20sum%20of%20relus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (main, Nov 24 2022, 08:09:04) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3e643153f50b336c1c7a9d4d544c5113a86fd55c72312d55d3acd153a8b13ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
